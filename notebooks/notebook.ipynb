{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "547ef3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97bbfd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = train_dataset.classes  # ['anomaly', 'normal']\n",
    "\n",
    "# data_iter = iter(train_loader)\n",
    "# images, labels = next(data_iter)\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# for i in range(16):\n",
    "#     plt.subplot(4, 4, i + 1)\n",
    "#     img = images[i].permute(1, 2, 0)\n",
    "#     label = class_names[labels[i]]\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(label)\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb21f87",
   "metadata": {},
   "source": [
    "# Method 1, flatten the input as a long array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "805bc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Check folder contents\n",
    "# import os\n",
    "\n",
    "# print(\"Train split:\")\n",
    "# for cls in (\"normal\",\"anomaly\"):\n",
    "#     p = f\"../split_anomaly_dataset/train/{cls}\"\n",
    "#     cnt = len([f for f in os.listdir(p) if f.lower().endswith(('.jpg','.png'))]) \\\n",
    "#           if os.path.isdir(p) else 0\n",
    "#     print(f\"  {cls:7s}: {cnt} files\")\n",
    "\n",
    "# # 2) Load with allow_empty\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# tf = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
    "# train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=tf, allow_empty=True)\n",
    "# print(\"Classes loaded:\", train_ds.classes)\n",
    "# print(\"Number of samples:\", len(train_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2655d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [markdown]\n",
    "# # # DAGMM Anomaly Detection with Sampled Anomalies in Train\n",
    "\n",
    "# # %% [code]\n",
    "# # 1) Make your DAGMM code importable\n",
    "# import sys, os\n",
    "# from pathlib import Path\n",
    "# sys.path.insert(0, str(Path.cwd().parent / \"Appropriate_dagmm\"))\n",
    "\n",
    "# # %% [code]\n",
    "# # 2) Imports\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms, datasets\n",
    "# from collections import Counter\n",
    "\n",
    "# from model import DAGMM  # your revised model.py\n",
    "\n",
    "# # %% [code]\n",
    "# # 3) Data transforms & loaders (images → flattened vectors)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((64,64)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=transform, allow_empty=True)\n",
    "# test_ds  = datasets.ImageFolder(\"../split_anomaly_dataset/test\",  transform=transform)\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# # Print ground‑truth counts\n",
    "# print(\"Train class counts:\", {train_ds.classes[k]: v for k,v in Counter(train_ds.targets).items()})\n",
    "# print(\"Test  class counts:\", {test_ds.classes[k]: v for k,v in Counter(test_ds.targets).items()})\n",
    "\n",
    "# # %% [code]\n",
    "# # 4) Model + optimizer\n",
    "# device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_features = 3 * 64 * 64\n",
    "\n",
    "# model = DAGMM(\n",
    "#     input_dim        = n_features,\n",
    "#     latent_dim       = 10,\n",
    "#     n_gmm_components = 5,\n",
    "#     comp_kwargs      = {'hidden_dims':[128,64], 'activation':torch.nn.Tanh},\n",
    "#     est_kwargs       = {'hidden_dims':[32],      'activation':torch.nn.ReLU, 'dropout':0.3},\n",
    "#     device           = device\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# # %% [code]\n",
    "# # 5) Training loop\n",
    "# n_epochs = 30\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for imgs, _ in train_loader:\n",
    "#         x = imgs.view(imgs.size(0), -1).to(device)  # flatten\n",
    "#         out  = model(x)\n",
    "#         loss = model.loss_function(x, out)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() * x.size(0)\n",
    "#     avg_loss = running_loss / len(train_loader.dataset)\n",
    "#     print(f\"Epoch {epoch}/{n_epochs} — avg train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# # %% [code]\n",
    "# # 6) Scoring test set & thresholding\n",
    "# model.eval()\n",
    "# energies = []\n",
    "# with torch.no_grad():\n",
    "#     for imgs, _ in test_loader:\n",
    "#         x = imgs.view(imgs.size(0), -1).to(device)\n",
    "#         energies.append(model(x)['energy'].cpu())\n",
    "# energies = torch.cat(energies)\n",
    "\n",
    "# # 95th‐percentile threshold\n",
    "# thr = energies.quantile(0.80)\n",
    "# mask = energies > thr\n",
    "# print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "\n",
    "# # %% [code]\n",
    "# # 7) (Optional) Visualize\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "# plt.axvline(thr, color='r', linestyle='--', label='95% threshold')\n",
    "# plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "760cc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# # 1) Recompute energies **and** collect true labels in the same order\n",
    "# model.eval()\n",
    "# energies = []\n",
    "# y_true   = []\n",
    "# with torch.no_grad():\n",
    "#     for imgs, labels in test_loader:\n",
    "#         x = imgs.view(imgs.size(0), -1).to(device)\n",
    "#         energies.append(model(x)['energy'].cpu())\n",
    "#         y_true.append(labels)\n",
    "# energies = torch.cat(energies)         # shape [N_test]\n",
    "# y_true   = torch.cat(y_true)           # shape [N_test]\n",
    "\n",
    "# # 2) Choose a threshold (you could sweep this on a val set)\n",
    "# thr = energies.quantile(0.95)\n",
    "\n",
    "# # 3) Build binary predictions: 1=anomaly, 0=normal\n",
    "# y_pred = (energies > thr).int()\n",
    "\n",
    "# # 4) Confusion matrix & classification report\n",
    "# print(\"Confusion matrix:\")\n",
    "# print(confusion_matrix(y_true, y_pred))\n",
    "# print(\"Accuracy\")\n",
    "# print(f\"Accuracy: {100 * (y_true == y_pred).float().mean():.2f}%\")\n",
    "# print(\"\\nClassification report:\")\n",
    "# print(classification_report(y_true, y_pred, target_names=test_ds.classes))\n",
    "\n",
    "# # 5) (Optional) AUC of the energy scores\n",
    "# auc = roc_auc_score(y_true, -energies)  # we invert since lower energy = more normal\n",
    "# print(f\"\\nROC‑AUC (energy as score): {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cadce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get top‑k highest‑energy samples\n",
    "# k = 5\n",
    "# idx = torch.topk(energies, k=k).indices\n",
    "\n",
    "# # grab their file paths & labels\n",
    "# for i in idx:\n",
    "#     path, label = test_ds.samples[i]\n",
    "#     print(f\"{path}  →  label={test_ds.classes[label]}, energy={energies[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc0910",
   "metadata": {},
   "source": [
    "# Method 2, use cnn based architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a840fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompressionNetwork from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/compression_network.py\n",
      "DAGMM   from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/model.py\n"
     ]
    }
   ],
   "source": [
    "# 0) Point to your CNN‑DAGMM folder *before* any imports\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "cnn_dir = Path.cwd().parent / \"cnn_dagmm\"\n",
    "sys.path.insert(0, str(cnn_dir))\n",
    "\n",
    "# 1) Import & force‑reload to clear any old cache\n",
    "import compression_network, model\n",
    "importlib.reload(compression_network)\n",
    "importlib.reload(model)\n",
    "\n",
    "# 2) Verify you’re using the right files\n",
    "print(\"CompressionNetwork from:\", compression_network.__file__)\n",
    "print(\"DAGMM   from:\",           model.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25f81c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts: {'normal': 284}\n",
      "Test  class counts: {'anomaly': 20, 'normal': 95}\n",
      "tensor(0.1607, grad_fn=<AddBackward0>) tensor(850.0509, grad_fn=<MeanBackward0>) tensor(459.9996, grad_fn=<SumBackward0>)\n",
      "tensor(0.1492, grad_fn=<AddBackward0>) tensor(794.5782, grad_fn=<MeanBackward0>) tensor(459.2724, grad_fn=<SumBackward0>)\n",
      "tensor(0.1193, grad_fn=<AddBackward0>) tensor(677.5420, grad_fn=<MeanBackward0>) tensor(458.4882, grad_fn=<SumBackward0>)\n",
      "tensor(0.1143, grad_fn=<AddBackward0>) tensor(615.7064, grad_fn=<MeanBackward0>) tensor(457.6715, grad_fn=<SumBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>) tensor(559.9345, grad_fn=<MeanBackward0>) tensor(456.8568, grad_fn=<SumBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>) tensor(524.4995, grad_fn=<MeanBackward0>) tensor(456.0298, grad_fn=<SumBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>) tensor(546.5284, grad_fn=<MeanBackward0>) tensor(455.1743, grad_fn=<SumBackward0>)\n",
      "tensor(0.0839, grad_fn=<AddBackward0>) tensor(472.1880, grad_fn=<MeanBackward0>) tensor(454.2816, grad_fn=<SumBackward0>)\n",
      "Epoch 1/50 — avg train loss: 304.7130\n",
      "tensor(0.0848, grad_fn=<AddBackward0>) tensor(457.1168, grad_fn=<MeanBackward0>) tensor(453.3435, grad_fn=<SumBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>) tensor(471.3648, grad_fn=<MeanBackward0>) tensor(452.3605, grad_fn=<SumBackward0>)\n",
      "tensor(0.0813, grad_fn=<AddBackward0>) tensor(435.0416, grad_fn=<MeanBackward0>) tensor(451.3300, grad_fn=<SumBackward0>)\n",
      "tensor(0.0786, grad_fn=<AddBackward0>) tensor(412.9998, grad_fn=<MeanBackward0>) tensor(450.2444, grad_fn=<SumBackward0>)\n",
      "tensor(0.0732, grad_fn=<AddBackward0>) tensor(376.9298, grad_fn=<MeanBackward0>) tensor(449.1025, grad_fn=<SumBackward0>)\n",
      "tensor(0.0746, grad_fn=<AddBackward0>) tensor(383.5061, grad_fn=<MeanBackward0>) tensor(447.9014, grad_fn=<SumBackward0>)\n",
      "tensor(0.0719, grad_fn=<AddBackward0>) tensor(369.3805, grad_fn=<MeanBackward0>) tensor(446.6396, grad_fn=<SumBackward0>)\n",
      "tensor(0.0697, grad_fn=<AddBackward0>) tensor(354.9651, grad_fn=<MeanBackward0>) tensor(445.3163, grad_fn=<SumBackward0>)\n",
      "Epoch 2/50 — avg train loss: 204.0663\n",
      "tensor(0.0654, grad_fn=<AddBackward0>) tensor(331.2499, grad_fn=<MeanBackward0>) tensor(443.9264, grad_fn=<SumBackward0>)\n",
      "tensor(0.0598, grad_fn=<AddBackward0>) tensor(299.6318, grad_fn=<MeanBackward0>) tensor(442.4689, grad_fn=<SumBackward0>)\n",
      "tensor(0.0651, grad_fn=<AddBackward0>) tensor(327.9693, grad_fn=<MeanBackward0>) tensor(440.9442, grad_fn=<SumBackward0>)\n",
      "tensor(0.0578, grad_fn=<AddBackward0>) tensor(294.4917, grad_fn=<MeanBackward0>) tensor(439.3507, grad_fn=<SumBackward0>)\n",
      "tensor(0.0574, grad_fn=<AddBackward0>) tensor(290.5816, grad_fn=<MeanBackward0>) tensor(437.6878, grad_fn=<SumBackward0>)\n",
      "tensor(0.0586, grad_fn=<AddBackward0>) tensor(297.0286, grad_fn=<MeanBackward0>) tensor(435.9557, grad_fn=<SumBackward0>)\n",
      "tensor(0.0529, grad_fn=<AddBackward0>) tensor(269.2849, grad_fn=<MeanBackward0>) tensor(434.1542, grad_fn=<SumBackward0>)\n",
      "tensor(0.0528, grad_fn=<AddBackward0>) tensor(268.2319, grad_fn=<MeanBackward0>) tensor(432.2840, grad_fn=<SumBackward0>)\n",
      "Epoch 3/50 — avg train loss: 153.8077\n",
      "tensor(0.0508, grad_fn=<AddBackward0>) tensor(260.2293, grad_fn=<MeanBackward0>) tensor(430.3468, grad_fn=<SumBackward0>)\n",
      "tensor(0.0526, grad_fn=<AddBackward0>) tensor(267.1664, grad_fn=<MeanBackward0>) tensor(428.3438, grad_fn=<SumBackward0>)\n",
      "tensor(0.0503, grad_fn=<AddBackward0>) tensor(259.2700, grad_fn=<MeanBackward0>) tensor(426.2764, grad_fn=<SumBackward0>)\n",
      "tensor(0.0468, grad_fn=<AddBackward0>) tensor(243.7516, grad_fn=<MeanBackward0>) tensor(424.1468, grad_fn=<SumBackward0>)\n",
      "tensor(0.0458, grad_fn=<AddBackward0>) tensor(242.7738, grad_fn=<MeanBackward0>) tensor(421.9556, grad_fn=<SumBackward0>)\n",
      "tensor(0.0471, grad_fn=<AddBackward0>) tensor(246.9685, grad_fn=<MeanBackward0>) tensor(419.7022, grad_fn=<SumBackward0>)\n",
      "tensor(0.0451, grad_fn=<AddBackward0>) tensor(241.4334, grad_fn=<MeanBackward0>) tensor(417.3896, grad_fn=<SumBackward0>)\n",
      "tensor(0.0456, grad_fn=<AddBackward0>) tensor(247.0619, grad_fn=<MeanBackward0>) tensor(415.0194, grad_fn=<SumBackward0>)\n",
      "Epoch 4/50 — avg train loss: 132.2671\n",
      "tensor(0.0440, grad_fn=<AddBackward0>) tensor(238.7832, grad_fn=<MeanBackward0>) tensor(412.5952, grad_fn=<SumBackward0>)\n",
      "tensor(0.0408, grad_fn=<AddBackward0>) tensor(225.0390, grad_fn=<MeanBackward0>) tensor(410.1194, grad_fn=<SumBackward0>)\n",
      "tensor(0.0414, grad_fn=<AddBackward0>) tensor(224.1971, grad_fn=<MeanBackward0>) tensor(407.5937, grad_fn=<SumBackward0>)\n",
      "tensor(0.0402, grad_fn=<AddBackward0>) tensor(217.2297, grad_fn=<MeanBackward0>) tensor(405.0232, grad_fn=<SumBackward0>)\n",
      "tensor(0.0442, grad_fn=<AddBackward0>) tensor(234.0670, grad_fn=<MeanBackward0>) tensor(402.4121, grad_fn=<SumBackward0>)\n",
      "tensor(0.0371, grad_fn=<AddBackward0>) tensor(203.6205, grad_fn=<MeanBackward0>) tensor(399.7646, grad_fn=<SumBackward0>)\n",
      "tensor(0.0397, grad_fn=<AddBackward0>) tensor(213.8930, grad_fn=<MeanBackward0>) tensor(397.0842, grad_fn=<SumBackward0>)\n",
      "tensor(0.0416, grad_fn=<AddBackward0>) tensor(225.0222, grad_fn=<MeanBackward0>) tensor(394.3743, grad_fn=<SumBackward0>)\n",
      "Epoch 5/50 — avg train loss: 118.6144\n",
      "tensor(0.0416, grad_fn=<AddBackward0>) tensor(223.9629, grad_fn=<MeanBackward0>) tensor(391.6383, grad_fn=<SumBackward0>)\n",
      "tensor(0.0367, grad_fn=<AddBackward0>) tensor(199.6619, grad_fn=<MeanBackward0>) tensor(388.8807, grad_fn=<SumBackward0>)\n",
      "tensor(0.0359, grad_fn=<AddBackward0>) tensor(196.0940, grad_fn=<MeanBackward0>) tensor(386.1052, grad_fn=<SumBackward0>)\n",
      "tensor(0.0405, grad_fn=<AddBackward0>) tensor(214.5468, grad_fn=<MeanBackward0>) tensor(383.3145, grad_fn=<SumBackward0>)\n",
      "tensor(0.0399, grad_fn=<AddBackward0>) tensor(215.6242, grad_fn=<MeanBackward0>) tensor(380.5119, grad_fn=<SumBackward0>)\n",
      "tensor(0.0375, grad_fn=<AddBackward0>) tensor(203.0006, grad_fn=<MeanBackward0>) tensor(377.7020, grad_fn=<SumBackward0>)\n",
      "tensor(0.0356, grad_fn=<AddBackward0>) tensor(195.7331, grad_fn=<MeanBackward0>) tensor(374.8876, grad_fn=<SumBackward0>)\n",
      "tensor(0.0371, grad_fn=<AddBackward0>) tensor(200.9012, grad_fn=<MeanBackward0>) tensor(372.0712, grad_fn=<SumBackward0>)\n",
      "Epoch 6/50 — avg train loss: 110.1772\n",
      "tensor(0.0346, grad_fn=<AddBackward0>) tensor(188.2437, grad_fn=<MeanBackward0>) tensor(369.2562, grad_fn=<SumBackward0>)\n",
      "tensor(0.0354, grad_fn=<AddBackward0>) tensor(193.7503, grad_fn=<MeanBackward0>) tensor(366.4466, grad_fn=<SumBackward0>)\n",
      "tensor(0.0356, grad_fn=<AddBackward0>) tensor(192.0915, grad_fn=<MeanBackward0>) tensor(363.6433, grad_fn=<SumBackward0>)\n",
      "tensor(0.0403, grad_fn=<AddBackward0>) tensor(213.2814, grad_fn=<MeanBackward0>) tensor(360.8505, grad_fn=<SumBackward0>)\n",
      "tensor(0.0370, grad_fn=<AddBackward0>) tensor(201.9544, grad_fn=<MeanBackward0>) tensor(358.0703, grad_fn=<SumBackward0>)\n",
      "tensor(0.0358, grad_fn=<AddBackward0>) tensor(196.9412, grad_fn=<MeanBackward0>) tensor(355.3041, grad_fn=<SumBackward0>)\n",
      "tensor(0.0360, grad_fn=<AddBackward0>) tensor(195.8178, grad_fn=<MeanBackward0>) tensor(352.5547, grad_fn=<SumBackward0>)\n",
      "tensor(0.0368, grad_fn=<AddBackward0>) tensor(200.9812, grad_fn=<MeanBackward0>) tensor(349.8245, grad_fn=<SumBackward0>)\n",
      "Epoch 7/50 — avg train loss: 105.4220\n",
      "tensor(0.0354, grad_fn=<AddBackward0>) tensor(193.2850, grad_fn=<MeanBackward0>) tensor(347.1159, grad_fn=<SumBackward0>)\n",
      "tensor(0.0345, grad_fn=<AddBackward0>) tensor(186.9912, grad_fn=<MeanBackward0>) tensor(344.4314, grad_fn=<SumBackward0>)\n",
      "tensor(0.0363, grad_fn=<AddBackward0>) tensor(197.8711, grad_fn=<MeanBackward0>) tensor(341.7714, grad_fn=<SumBackward0>)\n",
      "tensor(0.0346, grad_fn=<AddBackward0>) tensor(185.7397, grad_fn=<MeanBackward0>) tensor(339.1378, grad_fn=<SumBackward0>)\n",
      "tensor(0.0358, grad_fn=<AddBackward0>) tensor(192.4944, grad_fn=<MeanBackward0>) tensor(336.5331, grad_fn=<SumBackward0>)\n",
      "tensor(0.0344, grad_fn=<AddBackward0>) tensor(186.0213, grad_fn=<MeanBackward0>) tensor(333.9585, grad_fn=<SumBackward0>)\n",
      "tensor(0.0355, grad_fn=<AddBackward0>) tensor(192.6229, grad_fn=<MeanBackward0>) tensor(331.4136, grad_fn=<SumBackward0>)\n",
      "tensor(0.0346, grad_fn=<AddBackward0>) tensor(184.5017, grad_fn=<MeanBackward0>) tensor(328.9011, grad_fn=<SumBackward0>)\n",
      "Epoch 8/50 — avg train loss: 100.8685\n",
      "tensor(0.0366, grad_fn=<AddBackward0>) tensor(192.1942, grad_fn=<MeanBackward0>) tensor(326.4209, grad_fn=<SumBackward0>)\n",
      "tensor(0.0368, grad_fn=<AddBackward0>) tensor(192.8861, grad_fn=<MeanBackward0>) tensor(323.9747, grad_fn=<SumBackward0>)\n",
      "tensor(0.0326, grad_fn=<AddBackward0>) tensor(176.5355, grad_fn=<MeanBackward0>) tensor(321.5632, grad_fn=<SumBackward0>)\n",
      "tensor(0.0328, grad_fn=<AddBackward0>) tensor(177.4658, grad_fn=<MeanBackward0>) tensor(319.1870, grad_fn=<SumBackward0>)\n",
      "tensor(0.0341, grad_fn=<AddBackward0>) tensor(180.0636, grad_fn=<MeanBackward0>) tensor(316.8461, grad_fn=<SumBackward0>)\n",
      "tensor(0.0339, grad_fn=<AddBackward0>) tensor(180.8483, grad_fn=<MeanBackward0>) tensor(314.5419, grad_fn=<SumBackward0>)\n",
      "tensor(0.0323, grad_fn=<AddBackward0>) tensor(174.9298, grad_fn=<MeanBackward0>) tensor(312.2745, grad_fn=<SumBackward0>)\n",
      "tensor(0.0349, grad_fn=<AddBackward0>) tensor(185.7187, grad_fn=<MeanBackward0>) tensor(310.0447, grad_fn=<SumBackward0>)\n",
      "Epoch 9/50 — avg train loss: 96.6578\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(171.6036, grad_fn=<MeanBackward0>) tensor(307.8527, grad_fn=<SumBackward0>)\n",
      "tensor(0.0355, grad_fn=<AddBackward0>) tensor(185.2270, grad_fn=<MeanBackward0>) tensor(305.6981, grad_fn=<SumBackward0>)\n",
      "tensor(0.0328, grad_fn=<AddBackward0>) tensor(173.2366, grad_fn=<MeanBackward0>) tensor(303.5812, grad_fn=<SumBackward0>)\n",
      "tensor(0.0334, grad_fn=<AddBackward0>) tensor(179.1892, grad_fn=<MeanBackward0>) tensor(301.5026, grad_fn=<SumBackward0>)\n",
      "tensor(0.0368, grad_fn=<AddBackward0>) tensor(189.2610, grad_fn=<MeanBackward0>) tensor(299.4626, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(164.8695, grad_fn=<MeanBackward0>) tensor(297.4614, grad_fn=<SumBackward0>)\n",
      "tensor(0.0319, grad_fn=<AddBackward0>) tensor(170.3198, grad_fn=<MeanBackward0>) tensor(295.4962, grad_fn=<SumBackward0>)\n",
      "tensor(0.0350, grad_fn=<AddBackward0>) tensor(178.2671, grad_fn=<MeanBackward0>) tensor(293.5654, grad_fn=<SumBackward0>)\n",
      "Epoch 10/50 — avg train loss: 93.1253\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(162.7107, grad_fn=<MeanBackward0>) tensor(291.6711, grad_fn=<SumBackward0>)\n",
      "tensor(0.0336, grad_fn=<AddBackward0>) tensor(173.1143, grad_fn=<MeanBackward0>) tensor(289.8118, grad_fn=<SumBackward0>)\n",
      "tensor(0.0342, grad_fn=<AddBackward0>) tensor(176.4728, grad_fn=<MeanBackward0>) tensor(287.9886, grad_fn=<SumBackward0>)\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(168.0257, grad_fn=<MeanBackward0>) tensor(286.2010, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(162.5544, grad_fn=<MeanBackward0>) tensor(284.4481, grad_fn=<SumBackward0>)\n",
      "tensor(0.0366, grad_fn=<AddBackward0>) tensor(184.1190, grad_fn=<MeanBackward0>) tensor(282.7294, grad_fn=<SumBackward0>)\n",
      "tensor(0.0332, grad_fn=<AddBackward0>) tensor(169.5242, grad_fn=<MeanBackward0>) tensor(281.0450, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(161.9464, grad_fn=<MeanBackward0>) tensor(279.3928, grad_fn=<SumBackward0>)\n",
      "Epoch 11/50 — avg train loss: 89.4267\n",
      "tensor(0.0317, grad_fn=<AddBackward0>) tensor(163.5954, grad_fn=<MeanBackward0>) tensor(277.7726, grad_fn=<SumBackward0>)\n",
      "tensor(0.0320, grad_fn=<AddBackward0>) tensor(164.4584, grad_fn=<MeanBackward0>) tensor(276.1846, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(159.5856, grad_fn=<MeanBackward0>) tensor(274.6266, grad_fn=<SumBackward0>)\n",
      "tensor(0.0318, grad_fn=<AddBackward0>) tensor(161.7172, grad_fn=<MeanBackward0>) tensor(273.0984, grad_fn=<SumBackward0>)\n",
      "tensor(0.0328, grad_fn=<AddBackward0>) tensor(165.4136, grad_fn=<MeanBackward0>) tensor(271.5994, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(162.0085, grad_fn=<MeanBackward0>) tensor(270.1285, grad_fn=<SumBackward0>)\n",
      "tensor(0.0330, grad_fn=<AddBackward0>) tensor(163.5410, grad_fn=<MeanBackward0>) tensor(268.6857, grad_fn=<SumBackward0>)\n",
      "tensor(0.0355, grad_fn=<AddBackward0>) tensor(170.7255, grad_fn=<MeanBackward0>) tensor(267.2706, grad_fn=<SumBackward0>)\n",
      "Epoch 12/50 — avg train loss: 86.1691\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(155.9645, grad_fn=<MeanBackward0>) tensor(265.8832, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(162.6413, grad_fn=<MeanBackward0>) tensor(264.5221, grad_fn=<SumBackward0>)\n",
      "tensor(0.0330, grad_fn=<AddBackward0>) tensor(161.1238, grad_fn=<MeanBackward0>) tensor(263.1873, grad_fn=<SumBackward0>)\n",
      "tensor(0.0325, grad_fn=<AddBackward0>) tensor(158.8085, grad_fn=<MeanBackward0>) tensor(261.8788, grad_fn=<SumBackward0>)\n",
      "tensor(0.0312, grad_fn=<AddBackward0>) tensor(154.5354, grad_fn=<MeanBackward0>) tensor(260.5948, grad_fn=<SumBackward0>)\n",
      "tensor(0.0326, grad_fn=<AddBackward0>) tensor(158.1960, grad_fn=<MeanBackward0>) tensor(259.3360, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(154.3907, grad_fn=<MeanBackward0>) tensor(258.1022, grad_fn=<SumBackward0>)\n",
      "tensor(0.0325, grad_fn=<AddBackward0>) tensor(156.7371, grad_fn=<MeanBackward0>) tensor(256.8918, grad_fn=<SumBackward0>)\n",
      "Epoch 13/50 — avg train loss: 82.9267\n",
      "tensor(0.0328, grad_fn=<AddBackward0>) tensor(159.5297, grad_fn=<MeanBackward0>) tensor(255.7038, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(144.4247, grad_fn=<MeanBackward0>) tensor(254.5376, grad_fn=<SumBackward0>)\n",
      "tensor(0.0325, grad_fn=<AddBackward0>) tensor(157.3272, grad_fn=<MeanBackward0>) tensor(253.3916, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(149.5825, grad_fn=<MeanBackward0>) tensor(252.2657, grad_fn=<SumBackward0>)\n",
      "tensor(0.0329, grad_fn=<AddBackward0>) tensor(157.7083, grad_fn=<MeanBackward0>) tensor(251.1581, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(147.1555, grad_fn=<MeanBackward0>) tensor(250.0693, grad_fn=<SumBackward0>)\n",
      "tensor(0.0299, grad_fn=<AddBackward0>) tensor(145.1897, grad_fn=<MeanBackward0>) tensor(249.0004, grad_fn=<SumBackward0>)\n",
      "tensor(0.0329, grad_fn=<AddBackward0>) tensor(153.0582, grad_fn=<MeanBackward0>) tensor(247.9491, grad_fn=<SumBackward0>)\n",
      "Epoch 14/50 — avg train loss: 79.7683\n",
      "tensor(0.0316, grad_fn=<AddBackward0>) tensor(151.8063, grad_fn=<MeanBackward0>) tensor(246.9160, grad_fn=<SumBackward0>)\n",
      "tensor(0.0334, grad_fn=<AddBackward0>) tensor(152.9042, grad_fn=<MeanBackward0>) tensor(245.9013, grad_fn=<SumBackward0>)\n",
      "tensor(0.0332, grad_fn=<AddBackward0>) tensor(152.3301, grad_fn=<MeanBackward0>) tensor(244.9039, grad_fn=<SumBackward0>)\n",
      "tensor(0.0336, grad_fn=<AddBackward0>) tensor(156.3670, grad_fn=<MeanBackward0>) tensor(243.9230, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(136.6904, grad_fn=<MeanBackward0>) tensor(242.9587, grad_fn=<SumBackward0>)\n",
      "tensor(0.0316, grad_fn=<AddBackward0>) tensor(147.9318, grad_fn=<MeanBackward0>) tensor(242.0086, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(145.9458, grad_fn=<MeanBackward0>) tensor(241.0735, grad_fn=<SumBackward0>)\n",
      "tensor(0.0329, grad_fn=<AddBackward0>) tensor(149.2792, grad_fn=<MeanBackward0>) tensor(240.1519, grad_fn=<SumBackward0>)\n",
      "Epoch 15/50 — avg train loss: 78.2281\n",
      "tensor(0.0319, grad_fn=<AddBackward0>) tensor(148.0240, grad_fn=<MeanBackward0>) tensor(239.2445, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(144.4139, grad_fn=<MeanBackward0>) tensor(238.3501, grad_fn=<SumBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(143.0895, grad_fn=<MeanBackward0>) tensor(237.4669, grad_fn=<SumBackward0>)\n",
      "tensor(0.0329, grad_fn=<AddBackward0>) tensor(146.4500, grad_fn=<MeanBackward0>) tensor(236.5960, grad_fn=<SumBackward0>)\n",
      "tensor(0.0333, grad_fn=<AddBackward0>) tensor(151.8778, grad_fn=<MeanBackward0>) tensor(235.7366, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(140.1349, grad_fn=<MeanBackward0>) tensor(234.8896, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(141.8469, grad_fn=<MeanBackward0>) tensor(234.0539, grad_fn=<SumBackward0>)\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(135.0615, grad_fn=<MeanBackward0>) tensor(233.2298, grad_fn=<SumBackward0>)\n",
      "Epoch 16/50 — avg train loss: 75.5133\n",
      "tensor(0.0333, grad_fn=<AddBackward0>) tensor(142.7520, grad_fn=<MeanBackward0>) tensor(232.4133, grad_fn=<SumBackward0>)\n",
      "tensor(0.0312, grad_fn=<AddBackward0>) tensor(139.4504, grad_fn=<MeanBackward0>) tensor(231.6053, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(138.6718, grad_fn=<MeanBackward0>) tensor(230.8067, grad_fn=<SumBackward0>)\n",
      "tensor(0.0317, grad_fn=<AddBackward0>) tensor(139.1743, grad_fn=<MeanBackward0>) tensor(230.0169, grad_fn=<SumBackward0>)\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(134.5261, grad_fn=<MeanBackward0>) tensor(229.2363, grad_fn=<SumBackward0>)\n",
      "tensor(0.0311, grad_fn=<AddBackward0>) tensor(137.8227, grad_fn=<MeanBackward0>) tensor(228.4637, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(131.6841, grad_fn=<MeanBackward0>) tensor(227.6973, grad_fn=<SumBackward0>)\n",
      "tensor(0.0312, grad_fn=<AddBackward0>) tensor(136.0394, grad_fn=<MeanBackward0>) tensor(226.9376, grad_fn=<SumBackward0>)\n",
      "Epoch 17/50 — avg train loss: 72.3570\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(132.1564, grad_fn=<MeanBackward0>) tensor(226.1858, grad_fn=<SumBackward0>)\n",
      "tensor(0.0317, grad_fn=<AddBackward0>) tensor(137.0856, grad_fn=<MeanBackward0>) tensor(225.4438, grad_fn=<SumBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(133.7765, grad_fn=<MeanBackward0>) tensor(224.7123, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(125.2543, grad_fn=<MeanBackward0>) tensor(223.9898, grad_fn=<SumBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(131.5398, grad_fn=<MeanBackward0>) tensor(223.2715, grad_fn=<SumBackward0>)\n",
      "tensor(0.0329, grad_fn=<AddBackward0>) tensor(133.9102, grad_fn=<MeanBackward0>) tensor(222.5575, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(127.5155, grad_fn=<MeanBackward0>) tensor(221.8547, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(123.1889, grad_fn=<MeanBackward0>) tensor(221.1560, grad_fn=<SumBackward0>)\n",
      "Epoch 18/50 — avg train loss: 68.9486\n",
      "tensor(0.0330, grad_fn=<AddBackward0>) tensor(136.2640, grad_fn=<MeanBackward0>) tensor(220.4594, grad_fn=<SumBackward0>)\n",
      "tensor(0.0281, grad_fn=<AddBackward0>) tensor(119.4861, grad_fn=<MeanBackward0>) tensor(219.7689, grad_fn=<SumBackward0>)\n",
      "tensor(0.0294, grad_fn=<AddBackward0>) tensor(122.9159, grad_fn=<MeanBackward0>) tensor(219.0817, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(122.1095, grad_fn=<MeanBackward0>) tensor(218.3989, grad_fn=<SumBackward0>)\n",
      "tensor(0.0317, grad_fn=<AddBackward0>) tensor(127.6684, grad_fn=<MeanBackward0>) tensor(217.7351, grad_fn=<SumBackward0>)\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(132.6088, grad_fn=<MeanBackward0>) tensor(217.0804, grad_fn=<SumBackward0>)\n",
      "tensor(0.0308, grad_fn=<AddBackward0>) tensor(127.3385, grad_fn=<MeanBackward0>) tensor(216.4786, grad_fn=<SumBackward0>)\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(127.2479, grad_fn=<MeanBackward0>) tensor(215.8921, grad_fn=<SumBackward0>)\n",
      "Epoch 19/50 — avg train loss: 67.0774\n",
      "tensor(0.0339, grad_fn=<AddBackward0>) tensor(129.5025, grad_fn=<MeanBackward0>) tensor(215.3192, grad_fn=<SumBackward0>)\n",
      "tensor(0.0294, grad_fn=<AddBackward0>) tensor(118.6130, grad_fn=<MeanBackward0>) tensor(214.7574, grad_fn=<SumBackward0>)\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(120.7725, grad_fn=<MeanBackward0>) tensor(214.1962, grad_fn=<SumBackward0>)\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(119.9505, grad_fn=<MeanBackward0>) tensor(213.6356, grad_fn=<SumBackward0>)\n",
      "tensor(0.0293, grad_fn=<AddBackward0>) tensor(118.7682, grad_fn=<MeanBackward0>) tensor(213.0950, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(121.7224, grad_fn=<MeanBackward0>) tensor(212.5853, grad_fn=<SumBackward0>)\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(126.3611, grad_fn=<MeanBackward0>) tensor(212.1117, grad_fn=<SumBackward0>)\n",
      "tensor(0.0324, grad_fn=<AddBackward0>) tensor(124.8118, grad_fn=<MeanBackward0>) tensor(211.6622, grad_fn=<SumBackward0>)\n",
      "Epoch 20/50 — avg train loss: 64.8859\n",
      "tensor(0.0283, grad_fn=<AddBackward0>) tensor(115.0311, grad_fn=<MeanBackward0>) tensor(211.2168, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(115.9111, grad_fn=<MeanBackward0>) tensor(210.7747, grad_fn=<SumBackward0>)\n",
      "tensor(0.0362, grad_fn=<AddBackward0>) tensor(133.9891, grad_fn=<MeanBackward0>) tensor(210.3333, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(115.0198, grad_fn=<MeanBackward0>) tensor(209.8974, grad_fn=<SumBackward0>)\n",
      "tensor(0.0303, grad_fn=<AddBackward0>) tensor(116.5693, grad_fn=<MeanBackward0>) tensor(209.4669, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(112.1302, grad_fn=<MeanBackward0>) tensor(209.0370, grad_fn=<SumBackward0>)\n",
      "tensor(0.0303, grad_fn=<AddBackward0>) tensor(115.5513, grad_fn=<MeanBackward0>) tensor(208.6078, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(116.8363, grad_fn=<MeanBackward0>) tensor(208.1857, grad_fn=<SumBackward0>)\n",
      "Epoch 21/50 — avg train loss: 62.4944\n",
      "tensor(0.0313, grad_fn=<AddBackward0>) tensor(115.2403, grad_fn=<MeanBackward0>) tensor(207.7689, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(111.3726, grad_fn=<MeanBackward0>) tensor(207.3533, grad_fn=<SumBackward0>)\n",
      "tensor(0.0320, grad_fn=<AddBackward0>) tensor(117.6136, grad_fn=<MeanBackward0>) tensor(206.9401, grad_fn=<SumBackward0>)\n",
      "tensor(0.0299, grad_fn=<AddBackward0>) tensor(110.1152, grad_fn=<MeanBackward0>) tensor(206.5297, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(110.0598, grad_fn=<MeanBackward0>) tensor(206.1211, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(110.6732, grad_fn=<MeanBackward0>) tensor(205.7147, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(113.0654, grad_fn=<MeanBackward0>) tensor(205.3137, grad_fn=<SumBackward0>)\n",
      "tensor(0.0266, grad_fn=<AddBackward0>) tensor(101.3645, grad_fn=<MeanBackward0>) tensor(204.9134, grad_fn=<SumBackward0>)\n",
      "Epoch 22/50 — avg train loss: 59.4395\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(109.2844, grad_fn=<MeanBackward0>) tensor(204.5124, grad_fn=<SumBackward0>)\n",
      "tensor(0.0265, grad_fn=<AddBackward0>) tensor(99.9923, grad_fn=<MeanBackward0>) tensor(204.1195, grad_fn=<SumBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(108.1705, grad_fn=<MeanBackward0>) tensor(203.7248, grad_fn=<SumBackward0>)\n",
      "tensor(0.0316, grad_fn=<AddBackward0>) tensor(111.9113, grad_fn=<MeanBackward0>) tensor(203.3301, grad_fn=<SumBackward0>)\n",
      "tensor(0.0273, grad_fn=<AddBackward0>) tensor(102.8952, grad_fn=<MeanBackward0>) tensor(202.9472, grad_fn=<SumBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(111.9061, grad_fn=<MeanBackward0>) tensor(202.5700, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(106.8631, grad_fn=<MeanBackward0>) tensor(202.2215, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(103.0382, grad_fn=<MeanBackward0>) tensor(201.8793, grad_fn=<SumBackward0>)\n",
      "Epoch 23/50 — avg train loss: 57.2993\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(104.5439, grad_fn=<MeanBackward0>) tensor(201.5343, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(101.1644, grad_fn=<MeanBackward0>) tensor(201.1951, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(104.3629, grad_fn=<MeanBackward0>) tensor(200.8530, grad_fn=<SumBackward0>)\n",
      "tensor(0.0305, grad_fn=<AddBackward0>) tensor(106.5459, grad_fn=<MeanBackward0>) tensor(200.5181, grad_fn=<SumBackward0>)\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(102.7706, grad_fn=<MeanBackward0>) tensor(200.1832, grad_fn=<SumBackward0>)\n",
      "tensor(0.0291, grad_fn=<AddBackward0>) tensor(105.3454, grad_fn=<MeanBackward0>) tensor(199.8608, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(143.4379, grad_fn=<MeanBackward0>) tensor(199.5645, grad_fn=<SumBackward0>)\n",
      "tensor(0.0319, grad_fn=<AddBackward0>) tensor(200.9011, grad_fn=<MeanBackward0>) tensor(199.2812, grad_fn=<SumBackward0>)\n",
      "Epoch 24/50 — avg train loss: 63.6536\n",
      "tensor(0.0301, grad_fn=<AddBackward0>) tensor(120.3572, grad_fn=<MeanBackward0>) tensor(198.9998, grad_fn=<SumBackward0>)\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(114.1846, grad_fn=<MeanBackward0>) tensor(198.6991, grad_fn=<SumBackward0>)\n",
      "tensor(0.0308, grad_fn=<AddBackward0>) tensor(113.3406, grad_fn=<MeanBackward0>) tensor(198.3837, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(113.2033, grad_fn=<MeanBackward0>) tensor(198.0593, grad_fn=<SumBackward0>)\n",
      "tensor(0.0316, grad_fn=<AddBackward0>) tensor(116.8970, grad_fn=<MeanBackward0>) tensor(197.7313, grad_fn=<SumBackward0>)\n",
      "tensor(0.0307, grad_fn=<AddBackward0>) tensor(107.8027, grad_fn=<MeanBackward0>) tensor(197.3979, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(101.3587, grad_fn=<MeanBackward0>) tensor(197.0611, grad_fn=<SumBackward0>)\n",
      "tensor(0.0322, grad_fn=<AddBackward0>) tensor(107.7303, grad_fn=<MeanBackward0>) tensor(196.7256, grad_fn=<SumBackward0>)\n",
      "Epoch 25/50 — avg train loss: 59.3618\n",
      "tensor(0.0319, grad_fn=<AddBackward0>) tensor(106.1592, grad_fn=<MeanBackward0>) tensor(196.3958, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(103.0386, grad_fn=<MeanBackward0>) tensor(196.0740, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(96.2494, grad_fn=<MeanBackward0>) tensor(195.7579, grad_fn=<SumBackward0>)\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(100.0930, grad_fn=<MeanBackward0>) tensor(195.4469, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(99.8855, grad_fn=<MeanBackward0>) tensor(195.1422, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(102.2581, grad_fn=<MeanBackward0>) tensor(194.8451, grad_fn=<SumBackward0>)\n",
      "tensor(0.0280, grad_fn=<AddBackward0>) tensor(96.2889, grad_fn=<MeanBackward0>) tensor(194.5532, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(96.4035, grad_fn=<MeanBackward0>) tensor(194.2682, grad_fn=<SumBackward0>)\n",
      "Epoch 26/50 — avg train loss: 53.9212\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(98.8035, grad_fn=<MeanBackward0>) tensor(193.9902, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(94.2893, grad_fn=<MeanBackward0>) tensor(193.7199, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(94.5204, grad_fn=<MeanBackward0>) tensor(193.4552, grad_fn=<SumBackward0>)\n",
      "tensor(0.0299, grad_fn=<AddBackward0>) tensor(94.5429, grad_fn=<MeanBackward0>) tensor(193.1956, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(91.8993, grad_fn=<MeanBackward0>) tensor(192.9413, grad_fn=<SumBackward0>)\n",
      "tensor(0.0277, grad_fn=<AddBackward0>) tensor(91.3130, grad_fn=<MeanBackward0>) tensor(192.6912, grad_fn=<SumBackward0>)\n",
      "tensor(0.0295, grad_fn=<AddBackward0>) tensor(92.3253, grad_fn=<MeanBackward0>) tensor(192.4484, grad_fn=<SumBackward0>)\n",
      "tensor(0.0343, grad_fn=<AddBackward0>) tensor(101.3521, grad_fn=<MeanBackward0>) tensor(192.2091, grad_fn=<SumBackward0>)\n",
      "Epoch 27/50 — avg train loss: 51.4923\n",
      "tensor(0.0293, grad_fn=<AddBackward0>) tensor(91.7951, grad_fn=<MeanBackward0>) tensor(191.9721, grad_fn=<SumBackward0>)\n",
      "tensor(0.0314, grad_fn=<AddBackward0>) tensor(96.1336, grad_fn=<MeanBackward0>) tensor(191.7428, grad_fn=<SumBackward0>)\n",
      "tensor(0.0274, grad_fn=<AddBackward0>) tensor(90.7630, grad_fn=<MeanBackward0>) tensor(191.5222, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(91.0904, grad_fn=<MeanBackward0>) tensor(191.3165, grad_fn=<SumBackward0>)\n",
      "tensor(0.0305, grad_fn=<AddBackward0>) tensor(92.2774, grad_fn=<MeanBackward0>) tensor(191.1130, grad_fn=<SumBackward0>)\n",
      "tensor(0.0283, grad_fn=<AddBackward0>) tensor(88.7425, grad_fn=<MeanBackward0>) tensor(190.9122, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(91.9123, grad_fn=<MeanBackward0>) tensor(190.7169, grad_fn=<SumBackward0>)\n",
      "tensor(0.0280, grad_fn=<AddBackward0>) tensor(89.7344, grad_fn=<MeanBackward0>) tensor(190.5336, grad_fn=<SumBackward0>)\n",
      "Epoch 28/50 — avg train loss: 49.9097\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(89.4014, grad_fn=<MeanBackward0>) tensor(190.3546, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(88.9352, grad_fn=<MeanBackward0>) tensor(190.1769, grad_fn=<SumBackward0>)\n",
      "tensor(0.0269, grad_fn=<AddBackward0>) tensor(84.9410, grad_fn=<MeanBackward0>) tensor(190.0030, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(88.0320, grad_fn=<MeanBackward0>) tensor(189.8326, grad_fn=<SumBackward0>)\n",
      "tensor(0.0317, grad_fn=<AddBackward0>) tensor(92.7272, grad_fn=<MeanBackward0>) tensor(189.6656, grad_fn=<SumBackward0>)\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(83.1383, grad_fn=<MeanBackward0>) tensor(189.5050, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(87.2660, grad_fn=<MeanBackward0>) tensor(189.3400, grad_fn=<SumBackward0>)\n",
      "tensor(0.0283, grad_fn=<AddBackward0>) tensor(85.1390, grad_fn=<MeanBackward0>) tensor(189.1754, grad_fn=<SumBackward0>)\n",
      "Epoch 29/50 — avg train loss: 47.9913\n",
      "tensor(0.0273, grad_fn=<AddBackward0>) tensor(83.4835, grad_fn=<MeanBackward0>) tensor(189.0155, grad_fn=<SumBackward0>)\n",
      "tensor(0.0259, grad_fn=<AddBackward0>) tensor(79.7599, grad_fn=<MeanBackward0>) tensor(188.8588, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(85.8588, grad_fn=<MeanBackward0>) tensor(188.6960, grad_fn=<SumBackward0>)\n",
      "tensor(0.0313, grad_fn=<AddBackward0>) tensor(87.7557, grad_fn=<MeanBackward0>) tensor(188.5312, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(85.9186, grad_fn=<MeanBackward0>) tensor(188.3705, grad_fn=<SumBackward0>)\n",
      "tensor(0.0277, grad_fn=<AddBackward0>) tensor(101.2124, grad_fn=<MeanBackward0>) tensor(188.2262, grad_fn=<SumBackward0>)\n",
      "tensor(0.0317, grad_fn=<AddBackward0>) tensor(113.6758, grad_fn=<MeanBackward0>) tensor(188.0967, grad_fn=<SumBackward0>)\n",
      "tensor(0.0290, grad_fn=<AddBackward0>) tensor(86.1061, grad_fn=<MeanBackward0>) tensor(187.9676, grad_fn=<SumBackward0>)\n",
      "Epoch 30/50 — avg train loss: 49.2963\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(90.2399, grad_fn=<MeanBackward0>) tensor(187.8343, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(81.9564, grad_fn=<MeanBackward0>) tensor(187.6955, grad_fn=<SumBackward0>)\n",
      "tensor(0.0303, grad_fn=<AddBackward0>) tensor(88.6986, grad_fn=<MeanBackward0>) tensor(187.5455, grad_fn=<SumBackward0>)\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(85.8169, grad_fn=<MeanBackward0>) tensor(187.3888, grad_fn=<SumBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>) tensor(82.6600, grad_fn=<MeanBackward0>) tensor(187.2276, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(85.4637, grad_fn=<MeanBackward0>) tensor(187.0671, grad_fn=<SumBackward0>)\n",
      "tensor(0.0290, grad_fn=<AddBackward0>) tensor(85.6665, grad_fn=<MeanBackward0>) tensor(186.9127, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(79.7507, grad_fn=<MeanBackward0>) tensor(186.7608, grad_fn=<SumBackward0>)\n",
      "Epoch 31/50 — avg train loss: 46.7919\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(84.2947, grad_fn=<MeanBackward0>) tensor(186.6117, grad_fn=<SumBackward0>)\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(78.4828, grad_fn=<MeanBackward0>) tensor(186.4614, grad_fn=<SumBackward0>)\n",
      "tensor(0.0305, grad_fn=<AddBackward0>) tensor(83.8658, grad_fn=<MeanBackward0>) tensor(186.3104, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(80.5785, grad_fn=<MeanBackward0>) tensor(186.1610, grad_fn=<SumBackward0>)\n",
      "tensor(0.0280, grad_fn=<AddBackward0>) tensor(77.2751, grad_fn=<MeanBackward0>) tensor(186.0139, grad_fn=<SumBackward0>)\n",
      "tensor(0.0281, grad_fn=<AddBackward0>) tensor(78.7299, grad_fn=<MeanBackward0>) tensor(185.8688, grad_fn=<SumBackward0>)\n",
      "tensor(0.0313, grad_fn=<AddBackward0>) tensor(83.9513, grad_fn=<MeanBackward0>) tensor(185.7258, grad_fn=<SumBackward0>)\n",
      "tensor(0.0253, grad_fn=<AddBackward0>) tensor(73.5964, grad_fn=<MeanBackward0>) tensor(185.5875, grad_fn=<SumBackward0>)\n",
      "Epoch 32/50 — avg train loss: 44.5131\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(80.3358, grad_fn=<MeanBackward0>) tensor(185.4510, grad_fn=<SumBackward0>)\n",
      "tensor(0.0277, grad_fn=<AddBackward0>) tensor(75.9501, grad_fn=<MeanBackward0>) tensor(185.3153, grad_fn=<SumBackward0>)\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(75.4220, grad_fn=<MeanBackward0>) tensor(185.1811, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(75.1703, grad_fn=<MeanBackward0>) tensor(185.0518, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(76.6938, grad_fn=<MeanBackward0>) tensor(184.9229, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(76.9559, grad_fn=<MeanBackward0>) tensor(184.7934, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(74.6322, grad_fn=<MeanBackward0>) tensor(184.6658, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(74.3037, grad_fn=<MeanBackward0>) tensor(184.5407, grad_fn=<SumBackward0>)\n",
      "Epoch 33/50 — avg train loss: 42.6989\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(74.9255, grad_fn=<MeanBackward0>) tensor(184.4175, grad_fn=<SumBackward0>)\n",
      "tensor(0.0267, grad_fn=<AddBackward0>) tensor(72.0093, grad_fn=<MeanBackward0>) tensor(184.2981, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(75.1920, grad_fn=<MeanBackward0>) tensor(184.1773, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(75.6740, grad_fn=<MeanBackward0>) tensor(184.0573, grad_fn=<SumBackward0>)\n",
      "tensor(0.0251, grad_fn=<AddBackward0>) tensor(70.7375, grad_fn=<MeanBackward0>) tensor(183.9475, grad_fn=<SumBackward0>)\n",
      "tensor(0.0264, grad_fn=<AddBackward0>) tensor(70.1086, grad_fn=<MeanBackward0>) tensor(183.8369, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(74.8533, grad_fn=<MeanBackward0>) tensor(183.7271, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(70.5284, grad_fn=<MeanBackward0>) tensor(183.6200, grad_fn=<SumBackward0>)\n",
      "Epoch 34/50 — avg train loss: 41.2214\n",
      "tensor(0.0270, grad_fn=<AddBackward0>) tensor(70.1141, grad_fn=<MeanBackward0>) tensor(183.5099, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(71.7986, grad_fn=<MeanBackward0>) tensor(183.3984, grad_fn=<SumBackward0>)\n",
      "tensor(0.0249, grad_fn=<AddBackward0>) tensor(66.0227, grad_fn=<MeanBackward0>) tensor(183.2882, grad_fn=<SumBackward0>)\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(73.1873, grad_fn=<MeanBackward0>) tensor(183.1768, grad_fn=<SumBackward0>)\n",
      "tensor(0.0258, grad_fn=<AddBackward0>) tensor(66.5704, grad_fn=<MeanBackward0>) tensor(183.0633, grad_fn=<SumBackward0>)\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(70.1213, grad_fn=<MeanBackward0>) tensor(182.9513, grad_fn=<SumBackward0>)\n",
      "tensor(0.0267, grad_fn=<AddBackward0>) tensor(71.0797, grad_fn=<MeanBackward0>) tensor(182.8440, grad_fn=<SumBackward0>)\n",
      "tensor(0.0290, grad_fn=<AddBackward0>) tensor(80.1507, grad_fn=<MeanBackward0>) tensor(182.7510, grad_fn=<SumBackward0>)\n",
      "Epoch 35/50 — avg train loss: 40.3371\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(68.3200, grad_fn=<MeanBackward0>) tensor(182.6597, grad_fn=<SumBackward0>)\n",
      "tensor(0.0301, grad_fn=<AddBackward0>) tensor(72.9998, grad_fn=<MeanBackward0>) tensor(182.5730, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(70.4718, grad_fn=<MeanBackward0>) tensor(182.4848, grad_fn=<SumBackward0>)\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(67.1287, grad_fn=<MeanBackward0>) tensor(182.3923, grad_fn=<SumBackward0>)\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(70.8871, grad_fn=<MeanBackward0>) tensor(182.3016, grad_fn=<SumBackward0>)\n",
      "tensor(0.0268, grad_fn=<AddBackward0>) tensor(70.9552, grad_fn=<MeanBackward0>) tensor(182.2117, grad_fn=<SumBackward0>)\n",
      "tensor(0.0258, grad_fn=<AddBackward0>) tensor(75.6966, grad_fn=<MeanBackward0>) tensor(182.1238, grad_fn=<SumBackward0>)\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(73.5569, grad_fn=<MeanBackward0>) tensor(182.0442, grad_fn=<SumBackward0>)\n",
      "Epoch 36/50 — avg train loss: 40.3568\n",
      "tensor(0.0267, grad_fn=<AddBackward0>) tensor(69.2688, grad_fn=<MeanBackward0>) tensor(181.9636, grad_fn=<SumBackward0>)\n",
      "tensor(0.0265, grad_fn=<AddBackward0>) tensor(65.8857, grad_fn=<MeanBackward0>) tensor(181.8822, grad_fn=<SumBackward0>)\n",
      "tensor(0.0286, grad_fn=<AddBackward0>) tensor(68.4082, grad_fn=<MeanBackward0>) tensor(181.7959, grad_fn=<SumBackward0>)\n",
      "tensor(0.0299, grad_fn=<AddBackward0>) tensor(73.9408, grad_fn=<MeanBackward0>) tensor(181.7039, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(75.8716, grad_fn=<MeanBackward0>) tensor(181.6099, grad_fn=<SumBackward0>)\n",
      "tensor(0.0259, grad_fn=<AddBackward0>) tensor(75.4194, grad_fn=<MeanBackward0>) tensor(181.5132, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(68.7005, grad_fn=<MeanBackward0>) tensor(181.4338, grad_fn=<SumBackward0>)\n",
      "tensor(0.0274, grad_fn=<AddBackward0>) tensor(69.1373, grad_fn=<MeanBackward0>) tensor(181.3491, grad_fn=<SumBackward0>)\n",
      "Epoch 37/50 — avg train loss: 40.1354\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(66.9235, grad_fn=<MeanBackward0>) tensor(181.2629, grad_fn=<SumBackward0>)\n",
      "tensor(0.0249, grad_fn=<AddBackward0>) tensor(64.9332, grad_fn=<MeanBackward0>) tensor(181.1765, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(70.1317, grad_fn=<MeanBackward0>) tensor(181.0893, grad_fn=<SumBackward0>)\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(67.6857, grad_fn=<MeanBackward0>) tensor(180.9971, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(66.8236, grad_fn=<MeanBackward0>) tensor(180.9012, grad_fn=<SumBackward0>)\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(65.2628, grad_fn=<MeanBackward0>) tensor(180.8077, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(67.4765, grad_fn=<MeanBackward0>) tensor(180.7216, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>) tensor(62.8245, grad_fn=<MeanBackward0>) tensor(180.6332, grad_fn=<SumBackward0>)\n",
      "Epoch 38/50 — avg train loss: 38.1556\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(65.0129, grad_fn=<MeanBackward0>) tensor(180.5422, grad_fn=<SumBackward0>)\n",
      "tensor(0.0276, grad_fn=<AddBackward0>) tensor(62.7626, grad_fn=<MeanBackward0>) tensor(180.4535, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(67.5315, grad_fn=<MeanBackward0>) tensor(180.3646, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(64.0454, grad_fn=<MeanBackward0>) tensor(180.2802, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(63.4675, grad_fn=<MeanBackward0>) tensor(180.1926, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(62.5589, grad_fn=<MeanBackward0>) tensor(180.1062, grad_fn=<SumBackward0>)\n",
      "tensor(0.0259, grad_fn=<AddBackward0>) tensor(59.9449, grad_fn=<MeanBackward0>) tensor(180.0188, grad_fn=<SumBackward0>)\n",
      "tensor(0.0260, grad_fn=<AddBackward0>) tensor(58.8487, grad_fn=<MeanBackward0>) tensor(179.9323, grad_fn=<SumBackward0>)\n",
      "Epoch 39/50 — avg train loss: 36.5522\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(61.0084, grad_fn=<MeanBackward0>) tensor(179.8454, grad_fn=<SumBackward0>)\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(61.6639, grad_fn=<MeanBackward0>) tensor(179.7563, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>) tensor(62.0488, grad_fn=<MeanBackward0>) tensor(179.6637, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(62.6533, grad_fn=<MeanBackward0>) tensor(179.5727, grad_fn=<SumBackward0>)\n",
      "tensor(0.0267, grad_fn=<AddBackward0>) tensor(106.4276, grad_fn=<MeanBackward0>) tensor(179.4904, grad_fn=<SumBackward0>)\n",
      "tensor(0.0275, grad_fn=<AddBackward0>) tensor(177.9431, grad_fn=<MeanBackward0>) tensor(179.4096, grad_fn=<SumBackward0>)\n",
      "tensor(0.0355, grad_fn=<AddBackward0>) tensor(219.2035, grad_fn=<MeanBackward0>) tensor(179.3142, grad_fn=<SumBackward0>)\n",
      "tensor(0.0334, grad_fn=<AddBackward0>) tensor(180.9383, grad_fn=<MeanBackward0>) tensor(179.1955, grad_fn=<SumBackward0>)\n",
      "Epoch 40/50 — avg train loss: 60.6185\n",
      "tensor(0.0351, grad_fn=<AddBackward0>) tensor(185.2686, grad_fn=<MeanBackward0>) tensor(179.0398, grad_fn=<SumBackward0>)\n",
      "tensor(0.0328, grad_fn=<AddBackward0>) tensor(174.6085, grad_fn=<MeanBackward0>) tensor(178.8465, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(91.9653, grad_fn=<MeanBackward0>) tensor(178.6250, grad_fn=<SumBackward0>)\n",
      "tensor(0.0338, grad_fn=<AddBackward0>) tensor(94.4712, grad_fn=<MeanBackward0>) tensor(178.3893, grad_fn=<SumBackward0>)\n",
      "tensor(0.0312, grad_fn=<AddBackward0>) tensor(86.4608, grad_fn=<MeanBackward0>) tensor(178.1495, grad_fn=<SumBackward0>)\n",
      "tensor(0.0363, grad_fn=<AddBackward0>) tensor(90.8835, grad_fn=<MeanBackward0>) tensor(177.9223, grad_fn=<SumBackward0>)\n",
      "tensor(0.0349, grad_fn=<AddBackward0>) tensor(95.3618, grad_fn=<MeanBackward0>) tensor(177.7007, grad_fn=<SumBackward0>)\n",
      "tensor(0.0342, grad_fn=<AddBackward0>) tensor(79.2485, grad_fn=<MeanBackward0>) tensor(177.4822, grad_fn=<SumBackward0>)\n",
      "Epoch 41/50 — avg train loss: 58.6715\n",
      "tensor(0.0349, grad_fn=<AddBackward0>) tensor(80.1205, grad_fn=<MeanBackward0>) tensor(177.2644, grad_fn=<SumBackward0>)\n",
      "tensor(0.0304, grad_fn=<AddBackward0>) tensor(71.7629, grad_fn=<MeanBackward0>) tensor(177.0493, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(70.3282, grad_fn=<MeanBackward0>) tensor(176.8419, grad_fn=<SumBackward0>)\n",
      "tensor(0.0320, grad_fn=<AddBackward0>) tensor(69.6213, grad_fn=<MeanBackward0>) tensor(176.6442, grad_fn=<SumBackward0>)\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(69.4057, grad_fn=<MeanBackward0>) tensor(176.4562, grad_fn=<SumBackward0>)\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(67.7542, grad_fn=<MeanBackward0>) tensor(176.2758, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(66.0025, grad_fn=<MeanBackward0>) tensor(176.1024, grad_fn=<SumBackward0>)\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(64.8177, grad_fn=<MeanBackward0>) tensor(175.9376, grad_fn=<SumBackward0>)\n",
      "Epoch 42/50 — avg train loss: 39.5251\n",
      "tensor(0.0318, grad_fn=<AddBackward0>) tensor(65.2574, grad_fn=<MeanBackward0>) tensor(175.7839, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(61.0096, grad_fn=<MeanBackward0>) tensor(175.6407, grad_fn=<SumBackward0>)\n",
      "tensor(0.0331, grad_fn=<AddBackward0>) tensor(71.0218, grad_fn=<MeanBackward0>) tensor(175.5076, grad_fn=<SumBackward0>)\n",
      "tensor(0.0311, grad_fn=<AddBackward0>) tensor(75.4856, grad_fn=<MeanBackward0>) tensor(175.3823, grad_fn=<SumBackward0>)\n",
      "tensor(0.0288, grad_fn=<AddBackward0>) tensor(65.0875, grad_fn=<MeanBackward0>) tensor(175.2814, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(74.1815, grad_fn=<MeanBackward0>) tensor(175.1875, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(68.1839, grad_fn=<MeanBackward0>) tensor(175.0901, grad_fn=<SumBackward0>)\n",
      "tensor(0.0294, grad_fn=<AddBackward0>) tensor(61.7522, grad_fn=<MeanBackward0>) tensor(174.9868, grad_fn=<SumBackward0>)\n",
      "Epoch 43/50 — avg train loss: 38.4647\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(66.2703, grad_fn=<MeanBackward0>) tensor(174.8858, grad_fn=<SumBackward0>)\n",
      "tensor(0.0320, grad_fn=<AddBackward0>) tensor(68.1424, grad_fn=<MeanBackward0>) tensor(174.7948, grad_fn=<SumBackward0>)\n",
      "tensor(0.0274, grad_fn=<AddBackward0>) tensor(59.3204, grad_fn=<MeanBackward0>) tensor(174.7081, grad_fn=<SumBackward0>)\n",
      "tensor(0.0294, grad_fn=<AddBackward0>) tensor(62.1721, grad_fn=<MeanBackward0>) tensor(174.6230, grad_fn=<SumBackward0>)\n",
      "tensor(0.0317, grad_fn=<AddBackward0>) tensor(62.3441, grad_fn=<MeanBackward0>) tensor(174.5403, grad_fn=<SumBackward0>)\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(57.1096, grad_fn=<MeanBackward0>) tensor(174.4601, grad_fn=<SumBackward0>)\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(55.4407, grad_fn=<MeanBackward0>) tensor(174.3833, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(56.4101, grad_fn=<MeanBackward0>) tensor(174.3092, grad_fn=<SumBackward0>)\n",
      "Epoch 44/50 — avg train loss: 35.3433\n",
      "tensor(0.0281, grad_fn=<AddBackward0>) tensor(55.8761, grad_fn=<MeanBackward0>) tensor(174.2370, grad_fn=<SumBackward0>)\n",
      "tensor(0.0273, grad_fn=<AddBackward0>) tensor(54.8794, grad_fn=<MeanBackward0>) tensor(174.1664, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(58.6422, grad_fn=<MeanBackward0>) tensor(174.0979, grad_fn=<SumBackward0>)\n",
      "tensor(0.0272, grad_fn=<AddBackward0>) tensor(54.8716, grad_fn=<MeanBackward0>) tensor(174.0306, grad_fn=<SumBackward0>)\n",
      "tensor(0.0281, grad_fn=<AddBackward0>) tensor(61.9909, grad_fn=<MeanBackward0>) tensor(173.9651, grad_fn=<SumBackward0>)\n",
      "tensor(0.0299, grad_fn=<AddBackward0>) tensor(81.3590, grad_fn=<MeanBackward0>) tensor(173.9109, grad_fn=<SumBackward0>)\n",
      "tensor(0.0258, grad_fn=<AddBackward0>) tensor(57.0320, grad_fn=<MeanBackward0>) tensor(173.8523, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(61.7628, grad_fn=<MeanBackward0>) tensor(173.7807, grad_fn=<SumBackward0>)\n",
      "Epoch 45/50 — avg train loss: 35.2716\n",
      "tensor(0.0322, grad_fn=<AddBackward0>) tensor(62.7790, grad_fn=<MeanBackward0>) tensor(173.6989, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>) tensor(56.4694, grad_fn=<MeanBackward0>) tensor(173.6097, grad_fn=<SumBackward0>)\n",
      "tensor(0.0286, grad_fn=<AddBackward0>) tensor(68.3016, grad_fn=<MeanBackward0>) tensor(173.5185, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(118.5988, grad_fn=<MeanBackward0>) tensor(173.4429, grad_fn=<SumBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>) tensor(58.1658, grad_fn=<MeanBackward0>) tensor(173.3547, grad_fn=<SumBackward0>)\n",
      "tensor(0.0281, grad_fn=<AddBackward0>) tensor(67.6609, grad_fn=<MeanBackward0>) tensor(173.2521, grad_fn=<SumBackward0>)\n",
      "tensor(0.0294, grad_fn=<AddBackward0>) tensor(72.9601, grad_fn=<MeanBackward0>) tensor(173.1433, grad_fn=<SumBackward0>)\n",
      "tensor(0.0333, grad_fn=<AddBackward0>) tensor(69.0155, grad_fn=<MeanBackward0>) tensor(173.0278, grad_fn=<SumBackward0>)\n",
      "Epoch 46/50 — avg train loss: 40.1761\n",
      "tensor(0.0300, grad_fn=<AddBackward0>) tensor(60.5531, grad_fn=<MeanBackward0>) tensor(172.9061, grad_fn=<SumBackward0>)\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(60.4033, grad_fn=<MeanBackward0>) tensor(172.7800, grad_fn=<SumBackward0>)\n",
      "tensor(0.0281, grad_fn=<AddBackward0>) tensor(76.1988, grad_fn=<MeanBackward0>) tensor(172.6533, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(62.4826, grad_fn=<MeanBackward0>) tensor(172.5261, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(57.8390, grad_fn=<MeanBackward0>) tensor(172.4012, grad_fn=<SumBackward0>)\n",
      "tensor(0.0282, grad_fn=<AddBackward0>) tensor(55.2806, grad_fn=<MeanBackward0>) tensor(172.2794, grad_fn=<SumBackward0>)\n",
      "tensor(0.0269, grad_fn=<AddBackward0>) tensor(54.3938, grad_fn=<MeanBackward0>) tensor(172.1614, grad_fn=<SumBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>) tensor(57.8806, grad_fn=<MeanBackward0>) tensor(172.0462, grad_fn=<SumBackward0>)\n",
      "Epoch 47/50 — avg train loss: 35.1252\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(54.6901, grad_fn=<MeanBackward0>) tensor(171.9326, grad_fn=<SumBackward0>)\n",
      "tensor(0.0258, grad_fn=<AddBackward0>) tensor(51.4210, grad_fn=<MeanBackward0>) tensor(171.8223, grad_fn=<SumBackward0>)\n",
      "tensor(0.0286, grad_fn=<AddBackward0>) tensor(53.9499, grad_fn=<MeanBackward0>) tensor(171.7165, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(56.1424, grad_fn=<MeanBackward0>) tensor(171.6146, grad_fn=<SumBackward0>)\n",
      "tensor(0.0276, grad_fn=<AddBackward0>) tensor(52.3721, grad_fn=<MeanBackward0>) tensor(171.5231, grad_fn=<SumBackward0>)\n",
      "tensor(0.0297, grad_fn=<AddBackward0>) tensor(53.5873, grad_fn=<MeanBackward0>) tensor(171.4367, grad_fn=<SumBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>) tensor(50.3963, grad_fn=<MeanBackward0>) tensor(171.3528, grad_fn=<SumBackward0>)\n",
      "tensor(0.0302, grad_fn=<AddBackward0>) tensor(55.0426, grad_fn=<MeanBackward0>) tensor(171.2757, grad_fn=<SumBackward0>)\n",
      "Epoch 48/50 — avg train loss: 31.8493\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(65.3208, grad_fn=<MeanBackward0>) tensor(171.2029, grad_fn=<SumBackward0>)\n",
      "tensor(0.0281, grad_fn=<AddBackward0>) tensor(69.3121, grad_fn=<MeanBackward0>) tensor(171.1460, grad_fn=<SumBackward0>)\n",
      "tensor(0.0262, grad_fn=<AddBackward0>) tensor(61.1545, grad_fn=<MeanBackward0>) tensor(171.0826, grad_fn=<SumBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>) tensor(58.4931, grad_fn=<MeanBackward0>) tensor(171.0100, grad_fn=<SumBackward0>)\n",
      "tensor(0.0295, grad_fn=<AddBackward0>) tensor(53.1984, grad_fn=<MeanBackward0>) tensor(170.9301, grad_fn=<SumBackward0>)\n",
      "tensor(0.0307, grad_fn=<AddBackward0>) tensor(54.7452, grad_fn=<MeanBackward0>) tensor(170.8480, grad_fn=<SumBackward0>)\n",
      "tensor(0.0299, grad_fn=<AddBackward0>) tensor(53.5038, grad_fn=<MeanBackward0>) tensor(170.7643, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(48.9992, grad_fn=<MeanBackward0>) tensor(170.6753, grad_fn=<SumBackward0>)\n",
      "Epoch 49/50 — avg train loss: 33.9129\n",
      "tensor(0.0270, grad_fn=<AddBackward0>) tensor(49.4953, grad_fn=<MeanBackward0>) tensor(170.5843, grad_fn=<SumBackward0>)\n",
      "tensor(0.0284, grad_fn=<AddBackward0>) tensor(52.2181, grad_fn=<MeanBackward0>) tensor(170.4906, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(51.5022, grad_fn=<MeanBackward0>) tensor(170.4023, grad_fn=<SumBackward0>)\n",
      "tensor(0.0298, grad_fn=<AddBackward0>) tensor(53.5927, grad_fn=<MeanBackward0>) tensor(170.3221, grad_fn=<SumBackward0>)\n",
      "tensor(0.0292, grad_fn=<AddBackward0>) tensor(51.6787, grad_fn=<MeanBackward0>) tensor(170.2448, grad_fn=<SumBackward0>)\n",
      "tensor(0.0280, grad_fn=<AddBackward0>) tensor(51.6864, grad_fn=<MeanBackward0>) tensor(170.1700, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(47.6176, grad_fn=<MeanBackward0>) tensor(170.0979, grad_fn=<SumBackward0>)\n",
      "tensor(0.0278, grad_fn=<AddBackward0>) tensor(52.4195, grad_fn=<MeanBackward0>) tensor(170.0245, grad_fn=<SumBackward0>)\n",
      "Epoch 50/50 — avg train loss: 30.8110\n",
      "Detected anomalies in test set: 35 / 115\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # DAGMM Anomaly Detection with Sampled Anomalies in Train\n",
    "\n",
    "# %% [code]\n",
    "# 1) Make your DAGMM code importable\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"cnn_dagmm\"))\n",
    "\n",
    "# %% [code]\n",
    "# 2) Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from collections import Counter\n",
    "\n",
    "from model import DAGMM  # your revised model.py\n",
    "\n",
    "# %% [code]\n",
    "# 3) Data transforms & loaders (images → flattened vectors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=transform, allow_empty=True)\n",
    "test_ds  = datasets.ImageFolder(\"../split_anomaly_dataset/test\",  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# Print ground‑truth counts\n",
    "print(\"Train class counts:\", {train_ds.classes[k]: v for k,v in Counter(train_ds.targets).items()})\n",
    "print(\"Test  class counts:\", {test_ds.classes[k]: v for k,v in Counter(test_ds.targets).items()})\n",
    "\n",
    "# %% [code]\n",
    "# 4) Model + optimizer\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_features = 3 * 64 * 64\n",
    "\n",
    "model = DAGMM(\n",
    "    input_dim        = 3 * 64 * 64,     # still required by signature but not forwarded to CompressionNetwork\n",
    "    latent_dim       = 90,\n",
    "    n_gmm_components = 5,\n",
    "    comp_kwargs      = {'latent_dim': 90},  # now cleanly matches CompressionNetwork\n",
    "    est_kwargs       = {'hidden_dims': [128], 'activation': torch.nn.Tanh, 'dropout': 0.3},\n",
    "    device           = device\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# %% [code]\n",
    "# 5) Training loop\n",
    "n_epochs = 50\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, _ in train_loader:\n",
    "        x = imgs.to(device) \n",
    "        out  = model(x)\n",
    "        loss = model.loss_function(x, out)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}/{n_epochs} — avg train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# %% [code]\n",
    "# 6) Scoring test set & thresholding\n",
    "model.eval()\n",
    "energies = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in test_loader:\n",
    "        x = imgs.to(device)\n",
    "        energies.append(model(x)['energy'].cpu())\n",
    "energies = torch.cat(energies)\n",
    "\n",
    "# 95th‐percentile threshold\n",
    "thr = energies.quantile(0.70)\n",
    "mask = energies > thr\n",
    "print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "# y_pred_inv = (~(energies > thr)).int() #we want 1 for normal, 0 for anomaly. High energy = anomaly. \n",
    "# y_pred = y_pred_inv.int() # 0 for normal, 1 for anomaly\n",
    "# %% [code]\n",
    "# 7) (Optional) Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "plt.axvline(thr, color='r', linestyle='--', label='66% threshold')\n",
    "plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c64e7a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for top 30% anomalies: 88.08541870117188\n",
      "\n",
      "Confusion matrix:\n",
      "[[11  9]\n",
      " [ 8 87]]\n",
      "\n",
      "Accuracy: 85.22%\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anomaly       0.58      0.55      0.56        20\n",
      "      normal       0.91      0.92      0.91        95\n",
      "\n",
      "    accuracy                           0.85       115\n",
      "   macro avg       0.74      0.73      0.74       115\n",
      "weighted avg       0.85      0.85      0.85       115\n",
      "\n",
      "\n",
      "ROC‑AUC (energy as score): 0.847\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# 1) Recompute energies and collect true labels\n",
    "model.eval()\n",
    "energies = []\n",
    "y_true   = []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        x = imgs.to(device)                       # CNN takes [B,3,64,64]\n",
    "        out = model(x)\n",
    "        energies.append(out['energy'].cpu())      # [B]\n",
    "        y_true.append(labels)\n",
    "energies = torch.cat(energies)                  # [N_test]\n",
    "y_true   = torch.cat(y_true)                    # [N_test]\n",
    "\n",
    "# 2) Identify top 30% highest‐energy samples as anomalies\n",
    "#    70th percentile cutoff → top 30% above this\n",
    "thr = energies.quantile(0.84)\n",
    "\n",
    "\n",
    "# 3) Predictions\n",
    "y_pred_inv = (~(energies > thr)).int() #we want 1 for normal, 0 for anomaly. High energy = anomaly. \n",
    "y_pred = y_pred_inv.int() # 0 for normal, 1 for anomaly\n",
    "# 4) Metrics\n",
    "print(\"Threshold for top 30% anomalies:\", thr.item())\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "acc = (y_true == y_pred).float().mean() * 100\n",
    "print(f\"\\nAccuracy: {acc:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=test_ds.classes))\n",
    "\n",
    "# 5) ROC‑AUC (still informative even though we fix the cutoff by proportion)\n",
    "auc = roc_auc_score(y_true, -energies)  # invert since lower energy = more normal\n",
    "print(f\"\\nROC‑AUC (energy as score): {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8c220e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [code]\n",
    "# # Hyperparameter grid search for DAGMM\n",
    "# import itertools\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms, datasets\n",
    "# from model import DAGMM\n",
    "\n",
    "# # 1) Data loaders (reuse from above)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((64,64)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# train_ds = datasets.ImageFolder(\"../split_anomaly_dataset/train\", transform=transform, allow_empty=True)\n",
    "# test_ds  = datasets.ImageFolder(\"../split_anomaly_dataset/test\",  transform=transform)\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 2) Define search space\n",
    "# param_grid = {\n",
    "#     \"latent_dim\":       [2, 5, 10],\n",
    "#     \"n_gmm_components\": [2, 4, 6],\n",
    "#     \"est_hidden\":       [[32], [64, 32]],\n",
    "#     \"dropout\":          [0.1, 0.3],\n",
    "#     \"lr\":               [1e-3, 1e-4],\n",
    "# }\n",
    "\n",
    "# # 3) Helper to train & evaluate one config\n",
    "# def run_experiment(cfg):\n",
    "#     # build model\n",
    "#     model = DAGMM(\n",
    "#         input_dim        = 3*64*64,\n",
    "#         latent_dim       = cfg[\"latent_dim\"],\n",
    "#         n_gmm_components = cfg[\"n_gmm_components\"],\n",
    "#         comp_kwargs      = {\"latent_dim\": cfg[\"latent_dim\"]},\n",
    "#         est_kwargs       = {\n",
    "#             \"input_dim\": cfg[\"latent_dim\"]+2,\n",
    "#             \"output_dim\": cfg[\"n_gmm_components\"],\n",
    "#             \"hidden_dims\": cfg[\"est_hidden\"],\n",
    "#             \"activation\": torch.nn.ReLU,\n",
    "#             \"dropout\": cfg[\"dropout\"]\n",
    "#         },\n",
    "#         device=device\n",
    "#     ).to(device)\n",
    "#     opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n",
    "\n",
    "#     # train for fixed epochs\n",
    "#     for epoch in range(1, 11):\n",
    "#         model.train()\n",
    "#         for imgs, _ in train_loader:\n",
    "#             x = imgs.to(device)\n",
    "#             out = model(x)\n",
    "#             loss = model.loss_function(x, out)\n",
    "#             opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "#     # evaluate: compute energy scores & threshold at 90th percentile\n",
    "#     model.eval()\n",
    "#     energies, y_true = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for imgs, labels in test_loader:\n",
    "#             x = imgs.to(device)\n",
    "#             energies.append(model(x)[\"energy\"].cpu())\n",
    "#             y_true.append(labels)\n",
    "#     energies = torch.cat(energies)\n",
    "#     y_true   = torch.cat(y_true)\n",
    "\n",
    "#     thr = energies.quantile(0.9)\n",
    "#     y_pred = (energies > thr).int()\n",
    "#     acc = (y_pred == y_true).float().mean().item()\n",
    "\n",
    "#     return acc\n",
    "\n",
    "# # 4) Run grid search\n",
    "# results = []\n",
    "# for vals in itertools.product(*param_grid.values()):\n",
    "#     cfg = dict(zip(param_grid.keys(), vals))\n",
    "#     acc = run_experiment(cfg)\n",
    "#     print(f\"Config {cfg} → accuracy {acc:.3f}\")\n",
    "#     results.append({**cfg, \"accuracy\": acc})\n",
    "\n",
    "# # 5) Summarize\n",
    "# df = pd.DataFrame(results)\n",
    "# print(\"\\nTop 5 configs by accuracy:\")\n",
    "# print(df.sort_values(\"accuracy\", ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09632df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(\"accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88341822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import matplotlib.pyplot as plt\n",
    "# from model import DAGMM\n",
    "\n",
    "# # 1) Synthesize 100 images\n",
    "# white_imgs   = torch.ones(80, 3, 64, 64)\n",
    "# colored_imgs = torch.zeros(20, 3, 64, 64); colored_imgs[:,0] = 1.0\n",
    "# labels       = torch.cat([torch.zeros(80), torch.ones(20)]).long()\n",
    "\n",
    "# # single dataset\n",
    "# dataset = TensorDataset(torch.cat([white_imgs, colored_imgs], 0), labels)\n",
    "\n",
    "# # train loader (shuffle)\n",
    "# train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# # eval loader (no shuffle)\n",
    "# eval_loader  = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# # 2) Build & train DAGMM\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = DAGMM(\n",
    "#     input_dim=3*64*64, latent_dim=2, n_gmm_components=2,\n",
    "#     comp_kwargs={'latent_dim':2},\n",
    "#     est_kwargs ={\n",
    "#         'input_dim': 2+2,\n",
    "#         'output_dim': 2,\n",
    "#         'hidden_dims':[4],\n",
    "#         'activation':torch.nn.ReLU,\n",
    "#         'dropout':0.1\n",
    "#     },\n",
    "#     device=device\n",
    "# ).to(device)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# # 3) Train on white only\n",
    "# for epoch in range(1, 51):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     n_whites = 0\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         mask = (y_batch == 0)\n",
    "#         if not mask.any():\n",
    "#             continue\n",
    "#         x = x_batch[mask].to(device)\n",
    "#         out = model(x)\n",
    "#         loss = model.loss_function(x, out)\n",
    "#         opt.zero_grad()\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "\n",
    "#         total_loss += loss.item() * x.size(0)\n",
    "#         n_whites   += x.size(0)\n",
    "\n",
    "#     if epoch % 10 == 0 and n_whites > 0:\n",
    "#         print(f\"Epoch {epoch:02d} — avg white-loss: {total_loss / n_whites:.4f}\")\n",
    "\n",
    "# # 4) Compute energies & collect labels\n",
    "# model.eval()\n",
    "# energies = []\n",
    "# labels_list = []\n",
    "# with torch.no_grad():\n",
    "#     for x_batch, y_batch in eval_loader:\n",
    "#         x = x_batch.to(device)\n",
    "#         out = model(x)\n",
    "#         energies.append(out['energy'].cpu())\n",
    "#         labels_list.append(y_batch)\n",
    "\n",
    "# energies = torch.cat(energies)     # [100]\n",
    "# labels   = torch.cat(labels_list)  # [100]\n",
    "\n",
    "# # 5) Inspect distributions by label\n",
    "# white_e = energies[labels == 0]\n",
    "# red_e   = energies[labels == 1]\n",
    "\n",
    "# print(\"White  μ±σ:\", white_e.mean().item(), white_e.std().item())\n",
    "# print(\"Red    μ±σ:\",   red_e.mean().item(),   red_e.std().item())\n",
    "\n",
    "# plt.hist(white_e, bins=20, alpha=0.6, label='white')\n",
    "# plt.hist(red_e,   bins=20, alpha=0.6, label='red')\n",
    "# plt.legend(); plt.show()\n",
    "\n",
    "# # 6) Mid-point threshold\n",
    "# thr = (white_e.mean() + red_e.mean()) / 2\n",
    "# pred = (energies > thr).int()\n",
    "\n",
    "# print(\"Detected anomalies:\", pred[labels==1].sum().item(), \"/ 20\")\n",
    "# print(\"False positives:   \", pred[labels==0].sum().item(), \"/ 80\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "959f0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import roc_curve, f1_score, precision_recall_fscore_support\n",
    "\n",
    "# # assume `energies` [100] and `labels` [100] (0=white,1=red)\n",
    "# y_true = labels.numpy()\n",
    "# y_score = energies.numpy()\n",
    "\n",
    "# # 1) ROC curve and optimal threshold at max (TPR–FPR)\n",
    "# fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "# opt_idx = np.argmax(tpr - fpr)\n",
    "# opt_thr = thresholds[opt_idx]\n",
    "# print(f\"Optimal threshold from ROC: {opt_thr:.4f} (TPR={tpr[opt_idx]:.2f}, FPR={fpr[opt_idx]:.2f})\")\n",
    "\n",
    "# # 2) Build predictions & compute metrics\n",
    "# y_pred = (energies > opt_thr).int().numpy()\n",
    "# tp, fp, fn, tn = (\n",
    "#     ((y_true==1)&(y_pred==1)).sum(),\n",
    "#     ((y_true==0)&(y_pred==1)).sum(),\n",
    "#     ((y_true==1)&(y_pred==0)).sum(),\n",
    "#     ((y_true==0)&(y_pred==0)).sum(),\n",
    "# )\n",
    "# print(\"Confusion matrix:\")\n",
    "# print(f\"  [[TP={tp}, FP={fp}]\\n   [FN={fn}, TN={tn}]]\")\n",
    "\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "# print(f\"\\nPrecision (white,red): {precision}\")\n",
    "# print(f\"Recall    (white,red): {recall}\")\n",
    "# print(f\" F1‑score (white,red): {f1}\")\n",
    "\n",
    "# # 3) (Optional) print overall F1\n",
    "# print(f\"\\nOverall F1: {f1_score(y_true, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8f3dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 7419.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 7494.11it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 1349.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "IMG_SIZE = 64\n",
    "NORMAL_SHAPES = ['circle', 'square']\n",
    "NORMAL_COLORS = ['red', 'green', 'blue']\n",
    "NUM_NORMAL = 200\n",
    "NUM_ANOMALY = 20\n",
    "OUTPUT_DIR = \"dataset\"\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Utility: Create folders\n",
    "def make_dirs():\n",
    "    for split in ['train', 'test']:\n",
    "        for cls in ['normal', 'anomalous']:\n",
    "            os.makedirs(os.path.join(OUTPUT_DIR, split, cls), exist_ok=True)\n",
    "\n",
    "# Generate a normal image\n",
    "def generate_normal_image():\n",
    "    img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), 'black')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    shape = random.choice(NORMAL_SHAPES)\n",
    "    color = random.choice(NORMAL_COLORS)\n",
    "    x0, y0, x1, y1 = 16, 16, 48, 48\n",
    "\n",
    "    if shape == 'circle':\n",
    "        draw.ellipse([x0, y0, x1, y1], fill=color)\n",
    "    elif shape == 'square':\n",
    "        draw.rectangle([x0, y0, x1, y1], fill=color)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Generate an anomalous image\n",
    "def generate_anomalous_image():\n",
    "    img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), 'black')\n",
    "    choice = random.choice(['noise', 'triangle'])\n",
    "\n",
    "    if choice == 'noise':\n",
    "        pixels = img.load()\n",
    "        for i in range(IMG_SIZE):\n",
    "            for j in range(IMG_SIZE):\n",
    "                val = random.randint(0, 255)\n",
    "                pixels[i, j] = (val, val, val)\n",
    "    else:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.polygon([(32, 10), (50, 50), (14, 50)], fill='white')\n",
    "    return img\n",
    "\n",
    "# Save image\n",
    "def save_image(img, split, cls, idx):\n",
    "    path = os.path.join(OUTPUT_DIR, split, cls, f\"{cls}_{idx:03d}.png\")\n",
    "    img.save(path)\n",
    "\n",
    "# Main generation logic\n",
    "def generate_dataset():\n",
    "    make_dirs()\n",
    "\n",
    "    # TRAIN: 100 normal\n",
    "    print(\"Generating training set...\")\n",
    "    for i in tqdm(range(100)):\n",
    "        img = generate_normal_image()\n",
    "        save_image(img, 'train', 'normal', i)\n",
    "\n",
    "    # TEST: 100 normal + 20 anomalous\n",
    "    print(\"Generating test set...\")\n",
    "    for i in tqdm(range(100)):\n",
    "        img = generate_normal_image()\n",
    "        save_image(img, 'test', 'normal', i)\n",
    "    for i in tqdm(range(20)):\n",
    "        img = generate_anomalous_image()\n",
    "        save_image(img, 'test', 'anomalous', i)\n",
    "\n",
    "generate_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
