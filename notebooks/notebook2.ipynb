{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ae04ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4860.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6023.36it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 5509.76it/s]\n"
     ]
    }
   ],
   "source": [
    "### code to generate toy dataset in the same folder. \n",
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "IMG_SIZE = 64\n",
    "NORMAL_SHAPES = ['circle', 'square']\n",
    "NORMAL_COLORS = ['red', 'green', 'blue']\n",
    "NUM_NORMAL = 200\n",
    "NUM_ANOMALY = 20\n",
    "OUTPUT_DIR = \"dataset\"\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Utility: Create folders\n",
    "def make_dirs():\n",
    "    for split in ['train', 'test']:\n",
    "        for cls in ['normal', 'anomalous']:\n",
    "            os.makedirs(os.path.join(OUTPUT_DIR, split, cls), exist_ok=True)\n",
    "\n",
    "# Generate a normal image\n",
    "def generate_normal_image():\n",
    "    img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), 'black')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    shape = random.choice(NORMAL_SHAPES)\n",
    "    color = random.choice(NORMAL_COLORS)\n",
    "    x0, y0, x1, y1 = 16, 16, 48, 48\n",
    "\n",
    "    if shape == 'circle':\n",
    "        draw.ellipse([x0, y0, x1, y1], fill=color)\n",
    "    elif shape == 'square':\n",
    "        draw.rectangle([x0, y0, x1, y1], fill=color)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Generate an anomalous image (plain white)\n",
    "def generate_anomalous_image():\n",
    "    return Image.new('RGB', (IMG_SIZE, IMG_SIZE), 'white')\n",
    "\n",
    "# Save image\n",
    "def save_image(img, split, cls, idx):\n",
    "    path = os.path.join(OUTPUT_DIR, split, cls, f\"{cls}_{idx:03d}.png\")\n",
    "    img.save(path)\n",
    "\n",
    "# Main generation logic\n",
    "def generate_dataset():\n",
    "    make_dirs()\n",
    "\n",
    "    # TRAIN: 100 normal\n",
    "    print(\"Generating training set...\")\n",
    "    for i in tqdm(range(100)):\n",
    "        img = generate_normal_image()\n",
    "        save_image(img, 'train', 'normal', i)\n",
    "\n",
    "    # TEST: 100 normal + 20 anomalous\n",
    "    print(\"Generating test set...\")\n",
    "    for i in tqdm(range(100)):\n",
    "        img = generate_normal_image()\n",
    "        save_image(img, 'test', 'normal', i)\n",
    "    for i in tqdm(range(20)):\n",
    "        img = generate_anomalous_image()\n",
    "        save_image(img, 'test', 'anomalous', i)\n",
    "\n",
    "generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d956fe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompressionNetwork from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/compression_network.py\n",
      "DAGMM   from: /Users/aryan/Desktop/Academics /Semester 4/Data science/Project/gmm_anomalydetection/cnn_dagmm/model.py\n"
     ]
    }
   ],
   "source": [
    "# 0) Point to your CNN‑DAGMM folder *before* any imports\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "cnn_dir = Path.cwd().parent / \"cnn_dagmm\"\n",
    "sys.path.insert(0, str(cnn_dir))\n",
    "\n",
    "# 1) Import & force‑reload to clear any old cache\n",
    "import compression_network, model\n",
    "importlib.reload(compression_network)\n",
    "importlib.reload(model)\n",
    "\n",
    "# 2) Verify you’re using the right files\n",
    "print(\"CompressionNetwork from:\", compression_network.__file__)\n",
    "print(\"DAGMM   from:\",           model.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49e4ae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts: {'normal': 100}\n",
      "Test  class counts: {'anomalous': 20, 'normal': 100}\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # DAGMM Anomaly Detection with Sampled Anomalies in Train\n",
    "\n",
    "# %% [code]\n",
    "# 1) Make your DAGMM code importable\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"cnn_dagmm\"))\n",
    "\n",
    "# %% [code]\n",
    "# 2) Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from collections import Counter\n",
    "\n",
    "from model import DAGMM  # your revised model.py\n",
    "\n",
    "# %% [code]\n",
    "# 3) Data transforms & loaders (images → flattened vectors)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(\"dataset/train\", transform=transform, allow_empty=True)\n",
    "test_ds  = datasets.ImageFolder(\"dataset/test\",  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# Print ground‑truth counts\n",
    "print(\"Train class counts:\", {train_ds.classes[k]: v for k,v in Counter(train_ds.targets).items()})\n",
    "print(\"Test  class counts:\", {test_ds.classes[k]: v for k,v in Counter(test_ds.targets).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca88aa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in train_loader:\n",
    "    # labels = labels.numpy()\n",
    "    print(imgs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6accced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3267, grad_fn=<AddBackward0>) tensor(1838.3203, grad_fn=<MeanBackward0>) tensor(551.9995, grad_fn=<SumBackward0>)\n",
      "tensor(0.2879, grad_fn=<AddBackward0>) tensor(1690.7438, grad_fn=<MeanBackward0>) tensor(551.1047, grad_fn=<SumBackward0>)\n",
      "tensor(0.2697, grad_fn=<AddBackward0>) tensor(1525.8197, grad_fn=<MeanBackward0>) tensor(550.1173, grad_fn=<SumBackward0>)\n",
      "Epoch 1/50 — avg train loss: 835.5159\n",
      "tensor(0.2518, grad_fn=<AddBackward0>) tensor(1413.3766, grad_fn=<MeanBackward0>) tensor(549.1574, grad_fn=<SumBackward0>)\n",
      "tensor(0.2384, grad_fn=<AddBackward0>) tensor(1349.9307, grad_fn=<MeanBackward0>) tensor(548.2299, grad_fn=<SumBackward0>)\n",
      "tensor(0.2281, grad_fn=<AddBackward0>) tensor(1295.0344, grad_fn=<MeanBackward0>) tensor(547.3163, grad_fn=<SumBackward0>)\n",
      "Epoch 2/50 — avg train loss: 675.8798\n",
      "tensor(0.2219, grad_fn=<AddBackward0>) tensor(1230.1422, grad_fn=<MeanBackward0>) tensor(546.4108, grad_fn=<SumBackward0>)\n",
      "tensor(0.2164, grad_fn=<AddBackward0>) tensor(1211.5247, grad_fn=<MeanBackward0>) tensor(545.5101, grad_fn=<SumBackward0>)\n",
      "tensor(0.2079, grad_fn=<AddBackward0>) tensor(1158.9272, grad_fn=<MeanBackward0>) tensor(544.6053, grad_fn=<SumBackward0>)\n",
      "Epoch 3/50 — avg train loss: 602.4862\n",
      "tensor(0.2008, grad_fn=<AddBackward0>) tensor(1107.1648, grad_fn=<MeanBackward0>) tensor(543.6901, grad_fn=<SumBackward0>)\n",
      "tensor(0.1957, grad_fn=<AddBackward0>) tensor(1064.4089, grad_fn=<MeanBackward0>) tensor(542.7576, grad_fn=<SumBackward0>)\n",
      "tensor(0.1949, grad_fn=<AddBackward0>) tensor(1049.1348, grad_fn=<MeanBackward0>) tensor(541.8012, grad_fn=<SumBackward0>)\n",
      "Epoch 4/50 — avg train loss: 541.5546\n",
      "tensor(0.1816, grad_fn=<AddBackward0>) tensor(982.6873, grad_fn=<MeanBackward0>) tensor(540.8117, grad_fn=<SumBackward0>)\n",
      "tensor(0.1802, grad_fn=<AddBackward0>) tensor(977.8702, grad_fn=<MeanBackward0>) tensor(539.7811, grad_fn=<SumBackward0>)\n",
      "tensor(0.1703, grad_fn=<AddBackward0>) tensor(901.2176, grad_fn=<MeanBackward0>) tensor(538.7024, grad_fn=<SumBackward0>)\n",
      "Epoch 5/50 — avg train loss: 483.9630\n",
      "tensor(0.1614, grad_fn=<AddBackward0>) tensor(865.1616, grad_fn=<MeanBackward0>) tensor(537.5696, grad_fn=<SumBackward0>)\n",
      "tensor(0.1643, grad_fn=<AddBackward0>) tensor(906.7169, grad_fn=<MeanBackward0>) tensor(536.3757, grad_fn=<SumBackward0>)\n",
      "tensor(0.1521, grad_fn=<AddBackward0>) tensor(832.4045, grad_fn=<MeanBackward0>) tensor(535.1096, grad_fn=<SumBackward0>)\n",
      "Epoch 6/50 — avg train loss: 442.5831\n",
      "tensor(0.1472, grad_fn=<AddBackward0>) tensor(812.1351, grad_fn=<MeanBackward0>) tensor(533.7645, grad_fn=<SumBackward0>)\n",
      "tensor(0.1415, grad_fn=<AddBackward0>) tensor(773.5277, grad_fn=<MeanBackward0>) tensor(532.3362, grad_fn=<SumBackward0>)\n",
      "tensor(0.1355, grad_fn=<AddBackward0>) tensor(728.6735, grad_fn=<MeanBackward0>) tensor(530.8247, grad_fn=<SumBackward0>)\n",
      "Epoch 7/50 — avg train loss: 395.9804\n",
      "tensor(0.1253, grad_fn=<AddBackward0>) tensor(665.3055, grad_fn=<MeanBackward0>) tensor(529.2316, grad_fn=<SumBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>) tensor(679.0582, grad_fn=<MeanBackward0>) tensor(527.5589, grad_fn=<SumBackward0>)\n",
      "tensor(0.1188, grad_fn=<AddBackward0>) tensor(620.5150, grad_fn=<MeanBackward0>) tensor(525.8080, grad_fn=<SumBackward0>)\n",
      "Epoch 8/50 — avg train loss: 339.8209\n",
      "tensor(0.1151, grad_fn=<AddBackward0>) tensor(598.1104, grad_fn=<MeanBackward0>) tensor(523.9804, grad_fn=<SumBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>) tensor(581.0795, grad_fn=<MeanBackward0>) tensor(522.0772, grad_fn=<SumBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>) tensor(519.6536, grad_fn=<MeanBackward0>) tensor(520.1000, grad_fn=<SumBackward0>)\n",
      "Epoch 9/50 — avg train loss: 296.9787\n",
      "tensor(0.1011, grad_fn=<AddBackward0>) tensor(520.8183, grad_fn=<MeanBackward0>) tensor(518.0496, grad_fn=<SumBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>) tensor(476.9648, grad_fn=<MeanBackward0>) tensor(515.9266, grad_fn=<SumBackward0>)\n",
      "tensor(0.0938, grad_fn=<AddBackward0>) tensor(485.1591, grad_fn=<MeanBackward0>) tensor(513.7314, grad_fn=<SumBackward0>)\n",
      "Epoch 10/50 — avg train loss: 262.1262\n",
      "tensor(0.0910, grad_fn=<AddBackward0>) tensor(476.3606, grad_fn=<MeanBackward0>) tensor(511.4647, grad_fn=<SumBackward0>)\n",
      "tensor(0.0893, grad_fn=<AddBackward0>) tensor(463.1426, grad_fn=<MeanBackward0>) tensor(509.1269, grad_fn=<SumBackward0>)\n",
      "tensor(0.0827, grad_fn=<AddBackward0>) tensor(424.3449, grad_fn=<MeanBackward0>) tensor(506.7193, grad_fn=<SumBackward0>)\n",
      "Epoch 11/50 — avg train loss: 242.7369\n",
      "tensor(0.0793, grad_fn=<AddBackward0>) tensor(414.9139, grad_fn=<MeanBackward0>) tensor(504.2450, grad_fn=<SumBackward0>)\n",
      "tensor(0.0737, grad_fn=<AddBackward0>) tensor(383.2512, grad_fn=<MeanBackward0>) tensor(501.7054, grad_fn=<SumBackward0>)\n",
      "tensor(0.0739, grad_fn=<AddBackward0>) tensor(388.5963, grad_fn=<MeanBackward0>) tensor(499.1030, grad_fn=<SumBackward0>)\n",
      "Epoch 12/50 — avg train loss: 214.0353\n",
      "tensor(0.0659, grad_fn=<AddBackward0>) tensor(345.8850, grad_fn=<MeanBackward0>) tensor(496.4399, grad_fn=<SumBackward0>)\n",
      "tensor(0.0750, grad_fn=<AddBackward0>) tensor(402.2109, grad_fn=<MeanBackward0>) tensor(493.7181, grad_fn=<SumBackward0>)\n",
      "tensor(0.0674, grad_fn=<AddBackward0>) tensor(360.1475, grad_fn=<MeanBackward0>) tensor(490.9423, grad_fn=<SumBackward0>)\n",
      "Epoch 13/50 — avg train loss: 201.0832\n",
      "tensor(0.0628, grad_fn=<AddBackward0>) tensor(335.3102, grad_fn=<MeanBackward0>) tensor(488.1161, grad_fn=<SumBackward0>)\n",
      "tensor(0.0606, grad_fn=<AddBackward0>) tensor(323.8990, grad_fn=<MeanBackward0>) tensor(485.2430, grad_fn=<SumBackward0>)\n",
      "tensor(0.0598, grad_fn=<AddBackward0>) tensor(320.1211, grad_fn=<MeanBackward0>) tensor(482.3276, grad_fn=<SumBackward0>)\n",
      "Epoch 14/50 — avg train loss: 180.0425\n",
      "tensor(0.0573, grad_fn=<AddBackward0>) tensor(306.8731, grad_fn=<MeanBackward0>) tensor(479.3748, grad_fn=<SumBackward0>)\n",
      "tensor(0.0520, grad_fn=<AddBackward0>) tensor(278.6763, grad_fn=<MeanBackward0>) tensor(476.3891, grad_fn=<SumBackward0>)\n",
      "tensor(0.0579, grad_fn=<AddBackward0>) tensor(310.6935, grad_fn=<MeanBackward0>) tensor(473.3753, grad_fn=<SumBackward0>)\n",
      "Epoch 15/50 — avg train loss: 166.3186\n",
      "tensor(0.0506, grad_fn=<AddBackward0>) tensor(272.1812, grad_fn=<MeanBackward0>) tensor(470.3387, grad_fn=<SumBackward0>)\n",
      "tensor(0.0511, grad_fn=<AddBackward0>) tensor(275.3724, grad_fn=<MeanBackward0>) tensor(467.2837, grad_fn=<SumBackward0>)\n",
      "tensor(0.0536, grad_fn=<AddBackward0>) tensor(291.4152, grad_fn=<MeanBackward0>) tensor(464.2144, grad_fn=<SumBackward0>)\n",
      "Epoch 16/50 — avg train loss: 156.7141\n",
      "tensor(0.0453, grad_fn=<AddBackward0>) tensor(245.6750, grad_fn=<MeanBackward0>) tensor(461.1351, grad_fn=<SumBackward0>)\n",
      "tensor(0.0538, grad_fn=<AddBackward0>) tensor(296.4114, grad_fn=<MeanBackward0>) tensor(458.0496, grad_fn=<SumBackward0>)\n",
      "tensor(0.0467, grad_fn=<AddBackward0>) tensor(257.9374, grad_fn=<MeanBackward0>) tensor(454.9612, grad_fn=<SumBackward0>)\n",
      "Epoch 17/50 — avg train loss: 150.0368\n",
      "tensor(0.0476, grad_fn=<AddBackward0>) tensor(262.8267, grad_fn=<MeanBackward0>) tensor(451.8738, grad_fn=<SumBackward0>)\n",
      "tensor(0.0456, grad_fn=<AddBackward0>) tensor(253.1510, grad_fn=<MeanBackward0>) tensor(448.7912, grad_fn=<SumBackward0>)\n",
      "tensor(0.0401, grad_fn=<AddBackward0>) tensor(222.7093, grad_fn=<MeanBackward0>) tensor(445.7169, grad_fn=<SumBackward0>)\n",
      "Epoch 18/50 — avg train loss: 139.7747\n",
      "tensor(0.0419, grad_fn=<AddBackward0>) tensor(233.6693, grad_fn=<MeanBackward0>) tensor(442.6541, grad_fn=<SumBackward0>)\n",
      "tensor(0.0443, grad_fn=<AddBackward0>) tensor(247.0945, grad_fn=<MeanBackward0>) tensor(439.6062, grad_fn=<SumBackward0>)\n",
      "tensor(0.0419, grad_fn=<AddBackward0>) tensor(234.3200, grad_fn=<MeanBackward0>) tensor(436.5761, grad_fn=<SumBackward0>)\n",
      "Epoch 19/50 — avg train loss: 135.5558\n",
      "tensor(0.0428, grad_fn=<AddBackward0>) tensor(239.8481, grad_fn=<MeanBackward0>) tensor(433.5667, grad_fn=<SumBackward0>)\n",
      "tensor(0.0374, grad_fn=<AddBackward0>) tensor(210.0404, grad_fn=<MeanBackward0>) tensor(430.5803, grad_fn=<SumBackward0>)\n",
      "tensor(0.0423, grad_fn=<AddBackward0>) tensor(237.0766, grad_fn=<MeanBackward0>) tensor(427.6189, grad_fn=<SumBackward0>)\n",
      "Epoch 20/50 — avg train loss: 130.6218\n",
      "tensor(0.0378, grad_fn=<AddBackward0>) tensor(214.3454, grad_fn=<MeanBackward0>) tensor(424.6850, grad_fn=<SumBackward0>)\n",
      "tensor(0.0373, grad_fn=<AddBackward0>) tensor(212.1511, grad_fn=<MeanBackward0>) tensor(421.7803, grad_fn=<SumBackward0>)\n",
      "tensor(0.0390, grad_fn=<AddBackward0>) tensor(221.2718, grad_fn=<MeanBackward0>) tensor(418.9065, grad_fn=<SumBackward0>)\n",
      "Epoch 21/50 — avg train loss: 123.9254\n",
      "tensor(0.0380, grad_fn=<AddBackward0>) tensor(216.2739, grad_fn=<MeanBackward0>) tensor(416.0656, grad_fn=<SumBackward0>)\n",
      "tensor(0.0341, grad_fn=<AddBackward0>) tensor(197.0781, grad_fn=<MeanBackward0>) tensor(413.2590, grad_fn=<SumBackward0>)\n",
      "tensor(0.0384, grad_fn=<AddBackward0>) tensor(218.0196, grad_fn=<MeanBackward0>) tensor(410.4877, grad_fn=<SumBackward0>)\n",
      "Epoch 22/50 — avg train loss: 120.8918\n",
      "tensor(0.0327, grad_fn=<AddBackward0>) tensor(189.6101, grad_fn=<MeanBackward0>) tensor(407.7537, grad_fn=<SumBackward0>)\n",
      "tensor(0.0377, grad_fn=<AddBackward0>) tensor(216.0983, grad_fn=<MeanBackward0>) tensor(405.0567, grad_fn=<SumBackward0>)\n",
      "tensor(0.0429, grad_fn=<AddBackward0>) tensor(242.7296, grad_fn=<MeanBackward0>) tensor(402.3983, grad_fn=<SumBackward0>)\n",
      "Epoch 23/50 — avg train loss: 123.2297\n",
      "tensor(0.0339, grad_fn=<AddBackward0>) tensor(196.9395, grad_fn=<MeanBackward0>) tensor(399.7788, grad_fn=<SumBackward0>)\n",
      "tensor(0.0334, grad_fn=<AddBackward0>) tensor(194.7160, grad_fn=<MeanBackward0>) tensor(397.1989, grad_fn=<SumBackward0>)\n",
      "tensor(0.0373, grad_fn=<AddBackward0>) tensor(213.5044, grad_fn=<MeanBackward0>) tensor(394.6581, grad_fn=<SumBackward0>)\n",
      "Epoch 24/50 — avg train loss: 115.9253\n",
      "tensor(0.0319, grad_fn=<AddBackward0>) tensor(187.1303, grad_fn=<MeanBackward0>) tensor(392.1581, grad_fn=<SumBackward0>)\n",
      "tensor(0.0334, grad_fn=<AddBackward0>) tensor(194.1636, grad_fn=<MeanBackward0>) tensor(389.6983, grad_fn=<SumBackward0>)\n",
      "tensor(0.0343, grad_fn=<AddBackward0>) tensor(198.3774, grad_fn=<MeanBackward0>) tensor(387.2795, grad_fn=<SumBackward0>)\n",
      "Epoch 25/50 — avg train loss: 111.4855\n",
      "tensor(0.0332, grad_fn=<AddBackward0>) tensor(192.2543, grad_fn=<MeanBackward0>) tensor(384.9022, grad_fn=<SumBackward0>)\n",
      "tensor(0.0333, grad_fn=<AddBackward0>) tensor(192.7768, grad_fn=<MeanBackward0>) tensor(382.5666, grad_fn=<SumBackward0>)\n",
      "tensor(0.0315, grad_fn=<AddBackward0>) tensor(183.9266, grad_fn=<MeanBackward0>) tensor(380.2729, grad_fn=<SumBackward0>)\n",
      "Epoch 26/50 — avg train loss: 109.4284\n",
      "tensor(0.0321, grad_fn=<AddBackward0>) tensor(187.7760, grad_fn=<MeanBackward0>) tensor(378.0207, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(175.1272, grad_fn=<MeanBackward0>) tensor(375.8092, grad_fn=<SumBackward0>)\n",
      "tensor(0.0308, grad_fn=<AddBackward0>) tensor(181.8736, grad_fn=<MeanBackward0>) tensor(373.6377, grad_fn=<SumBackward0>)\n",
      "Epoch 27/50 — avg train loss: 105.2334\n",
      "tensor(0.0307, grad_fn=<AddBackward0>) tensor(180.6994, grad_fn=<MeanBackward0>) tensor(371.5054, grad_fn=<SumBackward0>)\n",
      "tensor(0.0290, grad_fn=<AddBackward0>) tensor(172.4936, grad_fn=<MeanBackward0>) tensor(369.4125, grad_fn=<SumBackward0>)\n",
      "tensor(0.0310, grad_fn=<AddBackward0>) tensor(181.1493, grad_fn=<MeanBackward0>) tensor(367.3585, grad_fn=<SumBackward0>)\n",
      "Epoch 28/50 — avg train loss: 103.2562\n",
      "tensor(0.0285, grad_fn=<AddBackward0>) tensor(169.8820, grad_fn=<MeanBackward0>) tensor(365.3434, grad_fn=<SumBackward0>)\n",
      "tensor(0.0287, grad_fn=<AddBackward0>) tensor(171.2947, grad_fn=<MeanBackward0>) tensor(363.3667, grad_fn=<SumBackward0>)\n",
      "tensor(0.0277, grad_fn=<AddBackward0>) tensor(168.2000, grad_fn=<MeanBackward0>) tensor(361.4275, grad_fn=<SumBackward0>)\n",
      "Epoch 29/50 — avg train loss: 98.9696\n",
      "tensor(0.0271, grad_fn=<AddBackward0>) tensor(164.4043, grad_fn=<MeanBackward0>) tensor(359.5241, grad_fn=<SumBackward0>)\n",
      "tensor(0.0318, grad_fn=<AddBackward0>) tensor(183.2079, grad_fn=<MeanBackward0>) tensor(357.6561, grad_fn=<SumBackward0>)\n",
      "tensor(0.0257, grad_fn=<AddBackward0>) tensor(156.9751, grad_fn=<MeanBackward0>) tensor(355.8238, grad_fn=<SumBackward0>)\n",
      "Epoch 30/50 — avg train loss: 97.9291\n",
      "tensor(0.0286, grad_fn=<AddBackward0>) tensor(169.1649, grad_fn=<MeanBackward0>) tensor(354.0246, grad_fn=<SumBackward0>)\n",
      "tensor(0.0247, grad_fn=<AddBackward0>) tensor(151.2812, grad_fn=<MeanBackward0>) tensor(352.2584, grad_fn=<SumBackward0>)\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(159.4456, grad_fn=<MeanBackward0>) tensor(350.5243, grad_fn=<SumBackward0>)\n",
      "Epoch 31/50 — avg train loss: 93.7170\n",
      "tensor(0.0255, grad_fn=<AddBackward0>) tensor(155.0386, grad_fn=<MeanBackward0>) tensor(348.8207, grad_fn=<SumBackward0>)\n",
      "tensor(0.0259, grad_fn=<AddBackward0>) tensor(156.3434, grad_fn=<MeanBackward0>) tensor(347.1481, grad_fn=<SumBackward0>)\n",
      "tensor(0.0247, grad_fn=<AddBackward0>) tensor(151.0383, grad_fn=<MeanBackward0>) tensor(345.5064, grad_fn=<SumBackward0>)\n",
      "Epoch 32/50 — avg train loss: 90.6752\n",
      "tensor(0.0227, grad_fn=<AddBackward0>) tensor(140.4194, grad_fn=<MeanBackward0>) tensor(343.8952, grad_fn=<SumBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>) tensor(173.7557, grad_fn=<MeanBackward0>) tensor(342.3152, grad_fn=<SumBackward0>)\n",
      "tensor(0.0257, grad_fn=<AddBackward0>) tensor(155.0718, grad_fn=<MeanBackward0>) tensor(340.7626, grad_fn=<SumBackward0>)\n",
      "Epoch 33/50 — avg train loss: 91.5360\n",
      "tensor(0.0242, grad_fn=<AddBackward0>) tensor(149.1116, grad_fn=<MeanBackward0>) tensor(339.2367, grad_fn=<SumBackward0>)\n",
      "tensor(0.0225, grad_fn=<AddBackward0>) tensor(140.6886, grad_fn=<MeanBackward0>) tensor(337.7362, grad_fn=<SumBackward0>)\n",
      "tensor(0.0235, grad_fn=<AddBackward0>) tensor(145.4255, grad_fn=<MeanBackward0>) tensor(336.2607, grad_fn=<SumBackward0>)\n",
      "Epoch 34/50 — avg train loss: 85.8703\n",
      "tensor(0.0221, grad_fn=<AddBackward0>) tensor(140.7878, grad_fn=<MeanBackward0>) tensor(334.8099, grad_fn=<SumBackward0>)\n",
      "tensor(0.0223, grad_fn=<AddBackward0>) tensor(140.6869, grad_fn=<MeanBackward0>) tensor(333.3834, grad_fn=<SumBackward0>)\n",
      "tensor(0.0208, grad_fn=<AddBackward0>) tensor(134.7804, grad_fn=<MeanBackward0>) tensor(331.9818, grad_fn=<SumBackward0>)\n",
      "Epoch 35/50 — avg train loss: 82.6245\n",
      "tensor(0.0206, grad_fn=<AddBackward0>) tensor(133.3896, grad_fn=<MeanBackward0>) tensor(330.6038, grad_fn=<SumBackward0>)\n",
      "tensor(0.0219, grad_fn=<AddBackward0>) tensor(137.2811, grad_fn=<MeanBackward0>) tensor(329.2492, grad_fn=<SumBackward0>)\n",
      "tensor(0.0198, grad_fn=<AddBackward0>) tensor(127.4330, grad_fn=<MeanBackward0>) tensor(327.9189, grad_fn=<SumBackward0>)\n",
      "Epoch 36/50 — avg train loss: 79.5209\n",
      "tensor(0.0199, grad_fn=<AddBackward0>) tensor(128.0964, grad_fn=<MeanBackward0>) tensor(326.6131, grad_fn=<SumBackward0>)\n",
      "tensor(0.0196, grad_fn=<AddBackward0>) tensor(125.9029, grad_fn=<MeanBackward0>) tensor(325.3303, grad_fn=<SumBackward0>)\n",
      "tensor(0.0190, grad_fn=<AddBackward0>) tensor(125.2628, grad_fn=<MeanBackward0>) tensor(324.0698, grad_fn=<SumBackward0>)\n",
      "Epoch 37/50 — avg train loss: 76.3169\n",
      "tensor(0.0263, grad_fn=<AddBackward0>) tensor(151.1319, grad_fn=<MeanBackward0>) tensor(322.8294, grad_fn=<SumBackward0>)\n",
      "tensor(0.0192, grad_fn=<AddBackward0>) tensor(123.8053, grad_fn=<MeanBackward0>) tensor(321.6117, grad_fn=<SumBackward0>)\n",
      "tensor(0.0204, grad_fn=<AddBackward0>) tensor(127.9169, grad_fn=<MeanBackward0>) tensor(320.4148, grad_fn=<SumBackward0>)\n",
      "Epoch 38/50 — avg train loss: 79.9154\n",
      "tensor(0.0183, grad_fn=<AddBackward0>) tensor(122.0235, grad_fn=<MeanBackward0>) tensor(319.2372, grad_fn=<SumBackward0>)\n",
      "tensor(0.0180, grad_fn=<AddBackward0>) tensor(120.0174, grad_fn=<MeanBackward0>) tensor(318.0812, grad_fn=<SumBackward0>)\n",
      "tensor(0.0170, grad_fn=<AddBackward0>) tensor(117.6750, grad_fn=<MeanBackward0>) tensor(316.9421, grad_fn=<SumBackward0>)\n",
      "Epoch 39/50 — avg train loss: 72.8398\n",
      "tensor(0.0162, grad_fn=<AddBackward0>) tensor(113.8786, grad_fn=<MeanBackward0>) tensor(315.8182, grad_fn=<SumBackward0>)\n",
      "tensor(0.0225, grad_fn=<AddBackward0>) tensor(136.4880, grad_fn=<MeanBackward0>) tensor(314.7107, grad_fn=<SumBackward0>)\n",
      "tensor(0.0161, grad_fn=<AddBackward0>) tensor(112.6532, grad_fn=<MeanBackward0>) tensor(313.6250, grad_fn=<SumBackward0>)\n",
      "Epoch 40/50 — avg train loss: 73.2072\n",
      "tensor(0.0193, grad_fn=<AddBackward0>) tensor(126.1690, grad_fn=<MeanBackward0>) tensor(312.5581, grad_fn=<SumBackward0>)\n",
      "tensor(0.0160, grad_fn=<AddBackward0>) tensor(112.8829, grad_fn=<MeanBackward0>) tensor(311.5065, grad_fn=<SumBackward0>)\n",
      "tensor(0.0160, grad_fn=<AddBackward0>) tensor(112.8832, grad_fn=<MeanBackward0>) tensor(310.4688, grad_fn=<SumBackward0>)\n",
      "Epoch 41/50 — avg train loss: 71.2786\n",
      "tensor(0.0168, grad_fn=<AddBackward0>) tensor(115.3644, grad_fn=<MeanBackward0>) tensor(309.4513, grad_fn=<SumBackward0>)\n",
      "tensor(0.0158, grad_fn=<AddBackward0>) tensor(111.4331, grad_fn=<MeanBackward0>) tensor(308.4465, grad_fn=<SumBackward0>)\n",
      "tensor(0.0176, grad_fn=<AddBackward0>) tensor(117.4101, grad_fn=<MeanBackward0>) tensor(307.4563, grad_fn=<SumBackward0>)\n",
      "Epoch 42/50 — avg train loss: 69.8949\n",
      "tensor(0.0154, grad_fn=<AddBackward0>) tensor(109.5402, grad_fn=<MeanBackward0>) tensor(306.4800, grad_fn=<SumBackward0>)\n",
      "tensor(0.0166, grad_fn=<AddBackward0>) tensor(113.6790, grad_fn=<MeanBackward0>) tensor(305.5170, grad_fn=<SumBackward0>)\n",
      "tensor(0.0150, grad_fn=<AddBackward0>) tensor(106.2142, grad_fn=<MeanBackward0>) tensor(304.5655, grad_fn=<SumBackward0>)\n",
      "Epoch 43/50 — avg train loss: 67.3894\n",
      "tensor(0.0155, grad_fn=<AddBackward0>) tensor(108.3733, grad_fn=<MeanBackward0>) tensor(303.6264, grad_fn=<SumBackward0>)\n",
      "tensor(0.0151, grad_fn=<AddBackward0>) tensor(106.2812, grad_fn=<MeanBackward0>) tensor(302.7016, grad_fn=<SumBackward0>)\n",
      "tensor(0.0140, grad_fn=<AddBackward0>) tensor(101.5379, grad_fn=<MeanBackward0>) tensor(301.7897, grad_fn=<SumBackward0>)\n",
      "Epoch 44/50 — avg train loss: 65.1349\n",
      "tensor(0.0147, grad_fn=<AddBackward0>) tensor(104.9660, grad_fn=<MeanBackward0>) tensor(300.8879, grad_fn=<SumBackward0>)\n",
      "tensor(0.0145, grad_fn=<AddBackward0>) tensor(104.0936, grad_fn=<MeanBackward0>) tensor(300.0033, grad_fn=<SumBackward0>)\n",
      "tensor(0.0139, grad_fn=<AddBackward0>) tensor(103.1056, grad_fn=<MeanBackward0>) tensor(299.1369, grad_fn=<SumBackward0>)\n",
      "Epoch 45/50 — avg train loss: 64.3607\n",
      "tensor(0.0140, grad_fn=<AddBackward0>) tensor(105.8940, grad_fn=<MeanBackward0>) tensor(298.3119, grad_fn=<SumBackward0>)\n",
      "tensor(0.0138, grad_fn=<AddBackward0>) tensor(105.3689, grad_fn=<MeanBackward0>) tensor(297.5229, grad_fn=<SumBackward0>)\n",
      "tensor(0.0137, grad_fn=<AddBackward0>) tensor(100.5980, grad_fn=<MeanBackward0>) tensor(296.7664, grad_fn=<SumBackward0>)\n",
      "Epoch 46/50 — avg train loss: 64.1926\n",
      "tensor(0.0130, grad_fn=<AddBackward0>) tensor(98.3083, grad_fn=<MeanBackward0>) tensor(296.0113, grad_fn=<SumBackward0>)\n",
      "tensor(0.0134, grad_fn=<AddBackward0>) tensor(98.8986, grad_fn=<MeanBackward0>) tensor(295.2624, grad_fn=<SumBackward0>)\n",
      "tensor(0.0134, grad_fn=<AddBackward0>) tensor(99.5425, grad_fn=<MeanBackward0>) tensor(294.5187, grad_fn=<SumBackward0>)\n",
      "Epoch 47/50 — avg train loss: 61.6653\n",
      "tensor(0.0125, grad_fn=<AddBackward0>) tensor(94.6307, grad_fn=<MeanBackward0>) tensor(293.7845, grad_fn=<SumBackward0>)\n",
      "tensor(0.0138, grad_fn=<AddBackward0>) tensor(98.9068, grad_fn=<MeanBackward0>) tensor(293.0557, grad_fn=<SumBackward0>)\n",
      "tensor(0.0140, grad_fn=<AddBackward0>) tensor(100.7390, grad_fn=<MeanBackward0>) tensor(292.3337, grad_fn=<SumBackward0>)\n",
      "Epoch 48/50 — avg train loss: 61.1639\n",
      "tensor(0.0125, grad_fn=<AddBackward0>) tensor(93.9568, grad_fn=<MeanBackward0>) tensor(291.6189, grad_fn=<SumBackward0>)\n",
      "tensor(0.0126, grad_fn=<AddBackward0>) tensor(94.6924, grad_fn=<MeanBackward0>) tensor(290.9087, grad_fn=<SumBackward0>)\n",
      "tensor(0.0124, grad_fn=<AddBackward0>) tensor(93.9254, grad_fn=<MeanBackward0>) tensor(290.2086, grad_fn=<SumBackward0>)\n",
      "Epoch 49/50 — avg train loss: 59.1877\n",
      "tensor(0.0139, grad_fn=<AddBackward0>) tensor(99.0258, grad_fn=<MeanBackward0>) tensor(289.5138, grad_fn=<SumBackward0>)\n",
      "tensor(0.0117, grad_fn=<AddBackward0>) tensor(90.6448, grad_fn=<MeanBackward0>) tensor(288.8230, grad_fn=<SumBackward0>)\n",
      "tensor(0.0130, grad_fn=<AddBackward0>) tensor(94.3562, grad_fn=<MeanBackward0>) tensor(288.1400, grad_fn=<SumBackward0>)\n",
      "Epoch 50/50 — avg train loss: 59.3203\n",
      "Detected anomalies in test set: 32 / 120\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %% [code]\n",
    "# 4) Model + optimizer\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_features = 3 * 64 * 64\n",
    "\n",
    "model = DAGMM(\n",
    "    input_dim        = 3 * 64 * 64,     # still required by signature but not forwarded to CompressionNetwork\n",
    "    latent_dim       = 90,\n",
    "    n_gmm_components = 6,\n",
    "    comp_kwargs      = {'latent_dim': 90},  # now cleanly matches CompressionNetwork\n",
    "    est_kwargs       = {'hidden_dims': [128], 'activation': torch.nn.Tanh, 'dropout': 0.3},\n",
    "    device           = device\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# %% [code]\n",
    "# 5) Training loop\n",
    "n_epochs = 50\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, _ in train_loader:\n",
    "        x = imgs.to(device) \n",
    "        out  = model(x)\n",
    "        loss = model.loss_function(x, out)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}/{n_epochs} — avg train loss: {avg_loss:.4f}\")\n",
    "\n",
    "# %% [code]\n",
    "# 6) Scoring test set & thresholding\n",
    "model.eval()\n",
    "energies = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in test_loader:\n",
    "        x = imgs.to(device)\n",
    "        energies.append(model(x)['energy'].cpu())\n",
    "energies = torch.cat(energies)\n",
    "\n",
    "# 95th‐percentile threshold\n",
    "thr = energies.quantile(0.70)\n",
    "mask = energies > thr\n",
    "print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "\n",
    "# %% [code]\n",
    "# 7) (Optional) Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "plt.axvline(thr, color='r', linestyle='--', label='66% threshold')\n",
    "plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f89b9b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4324.2354, 4324.2354, 4324.2354, 4324.2354, 4324.2354, 4324.2354,\n",
      "        4324.2354, 4324.2354, 4324.2354, 4324.2354, 4324.2354, 4324.2354,\n",
      "        4324.2354, 4324.2354, 4324.2354, 4324.2354, 4324.2354, 4324.2354,\n",
      "        4324.2354, 4324.2354,   98.7730,   98.7730,   93.7499,  108.5247,\n",
      "          93.7499,  132.8648,   93.7499,  114.1434,   98.7730,  132.8648,\n",
      "         114.1434,  114.1434,  134.5560,  132.8648,   98.7730,  134.5560,\n",
      "          93.7499,   98.7730,  132.8648,   93.7499,   93.7499,  114.1434,\n",
      "          98.7730,  134.5560,   98.7730,   98.7730,   98.7730,  114.1434,\n",
      "          93.7499,   93.7499,  108.5247,  108.5247,  114.1434,  114.1434,\n",
      "          93.7499,  108.5247,   98.7730,  132.8648,   93.7499,  132.8648,\n",
      "         134.5560,   98.7730,   98.7730,  132.8648,  132.8648,  108.5247,\n",
      "         132.8648,  132.8648,  132.8648,  108.5247,  114.1434,  132.8648,\n",
      "          93.7499,  114.1434,  132.8648,  114.1434,   93.7499,  108.5247,\n",
      "          98.7730,  132.8648,   93.7499,   93.7499,  108.5247,   93.7499,\n",
      "         134.5560,  134.5560,  132.8648,  134.5560,   98.7730,   93.7499,\n",
      "         132.8648,  108.5247,  134.5560,  132.8648,   93.7499,  134.5560,\n",
      "         114.1434,   98.7730,  114.1434,   98.7730,   93.7499,   93.7499,\n",
      "         134.5560,  134.5560,   98.7730,  132.8648,  132.8648,  108.5247,\n",
      "         108.5247,   98.7730,  114.1434,  108.5247,  132.8648,  132.8648,\n",
      "         108.5247,  134.5560,   98.7730,   98.7730,   93.7499,  132.8648])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Detected anomalies in test set: 0 / 120\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# 6) Scoring test set & thresholding\n",
    "model.eval()\n",
    "energies = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in test_loader:\n",
    "        x = imgs.to(device)\n",
    "        energies.append(model(x)['energy'].cpu())\n",
    "energies = torch.cat(energies)\n",
    "\n",
    "# 95th‐percentile threshold\n",
    "thr = energies.quantile(0.90)\n",
    "mask = energies > thr\n",
    "print(energies)\n",
    "print(mask)\n",
    "print(f\"Detected anomalies in test set: {mask.sum().item()} / {len(energies)}\")\n",
    "\n",
    "# # %% [code]\n",
    "# # 7) (Optional) Visualize\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(energies.numpy(), bins=50, alpha=0.7)\n",
    "# plt.axvline(thr, color='r', linestyle='--', label='66% threshold')\n",
    "# plt.legend(); plt.title(\"Test Energy Distribution\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee114b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for top 30% anomalies: 134.5559844970703\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 20   0]\n",
      " [  0 100]]\n",
      "\n",
      "Accuracy: 100.00%\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   anomalous       1.00      1.00      1.00        20\n",
      "      normal       1.00      1.00      1.00       100\n",
      "\n",
      "    accuracy                           1.00       120\n",
      "   macro avg       1.00      1.00      1.00       120\n",
      "weighted avg       1.00      1.00      1.00       120\n",
      "\n",
      "\n",
      "ROC‑AUC (energy as score): 1.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# 1) Recompute energies and collect true labels\n",
    "model.eval()\n",
    "energies = []\n",
    "y_true   = []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        x = imgs.to(device)                       # CNN takes [B,3,64,64]\n",
    "        out = model(x)\n",
    "        energies.append(out['energy'].cpu())      # [B]\n",
    "        y_true.append(labels)\n",
    "energies = torch.cat(energies)                  # [N_test]\n",
    "y_true   = torch.cat(y_true)                    # [N_test]\n",
    "\n",
    "# 2) Identify top 30% highest‐energy samples as anomalies\n",
    "#    70th percentile cutoff → top 30% above this\n",
    "thr = energies.quantile(0.80)\n",
    "\n",
    "\n",
    "# 3) Predictions\n",
    "# 3) Logical not then cast\n",
    "y_pred_inv = (~(energies > thr)).int() #we want 1 for normal, 0 for anomaly. High energy = anomaly. \n",
    "y_pred = y_pred_inv.int() # 0 for normal, 1 for anomaly\n",
    "# 4) Metrics\n",
    "print(\"Threshold for top 30% anomalies:\", thr.item())\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "acc = (y_true == y_pred).float().mean() * 100\n",
    "print(f\"\\nAccuracy: {acc:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=test_ds.classes))\n",
    "\n",
    "# 5) ROC‑AUC (still informative even though we fix the cutoff by proportion)\n",
    "auc = roc_auc_score(y_true, -energies)  # invert since lower energy = more normal\n",
    "print(f\"\\nROC‑AUC (energy as score): {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3bbdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
